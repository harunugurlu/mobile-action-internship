{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eefa9e2a",
   "metadata": {},
   "source": [
    "# This notebook is a PoC attempt on memory usage optimization in Pandas\n",
    "\n",
    "### Contents:\n",
    "\n",
    "#####      1. Optimization directly on pandas dataframes\n",
    "#####      2. Using NumPy arrays vs Pandas DataFrames\n",
    "#####      3. Dask DataFrames vs Pandas DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c5c780",
   "metadata": {},
   "source": [
    "## 1. Optimization directly on pandas dataframes\n",
    "\n",
    "   #### a. Alter the column dtypes:\n",
    "    \n",
    "  When we create a Pandas DataFrame, Pandas will assign the highest memory datatype to columns by default. For example, when     it detects a column of integers it will assign ```int64``` to the dtype of the column, regardless of the size of the integer   values. This consumes a lot of unnecessary memory. We can use the ```astype()``` method to downgrade the datatypes. Here is     an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f72f233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51fea205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Method to generate dummy dataframe that we will use throughout the notebook.\n",
    "from itertools import cycle\n",
    "\n",
    "def generate_fake_dataframe(size, cols, col_names = None, intervals = None, seed = None):\n",
    "    \n",
    "    categories_dict = {'animals': ['cow', 'rabbit', 'duck', 'shrimp', 'pig', 'goat', 'crab', 'deer', 'bee', 'sheep', 'fish', 'turkey', 'dove', 'chicken', 'horse'],\n",
    "                       'names'  : ['James', 'Mary', 'Robert', 'Patricia', 'John', 'Jennifer', 'Michael', 'Linda', 'William', 'Elizabeth', 'Ahmed', 'Barbara', 'Richard', 'Susan', 'Salomon', 'Juan Luis'],\n",
    "                       'cities' : ['Stockholm', 'Denver', 'Moscow', 'Marseille', 'Palermo', 'Tokyo', 'Lisbon', 'Oslo', 'Nairobi', 'Río de Janeiro', 'Berlin', 'Bogotá', 'Manila', 'Madrid', 'Milwaukee'],\n",
    "                       'colors' : ['red', 'orange', 'yellow', 'green', 'blue', 'indigo', 'purple', 'pink', 'silver', 'gold', 'beige', 'brown', 'grey', 'black', 'white']\n",
    "                      }\n",
    "    default_intervals = {\"i\" : (0,10), \"f\" : (0,100), \"c\" : (\"names\", 5), \"d\" : (\"2020-01-01\",\"2020-12-31\")}\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    first_c = default_intervals[\"c\"][0]\n",
    "    categories_names = cycle([first_c] + [c for c in categories_dict.keys() if c != first_c])\n",
    "    default_intervals[\"c\"] = (categories_names, default_intervals[\"c\"][1])\n",
    "    \n",
    "    if isinstance(col_names,list):\n",
    "        assert len(col_names) == len(cols), f\"The fake DataFrame should have {len(cols)} columns but col_names is a list with {len(col_names)} elements\"\n",
    "    elif col_names is None:\n",
    "        suffix = {\"c\" : \"cat\", \"i\" : \"int\", \"f\" : \"float\", \"d\" : \"date\"}\n",
    "        col_names = [f\"column_{str(i)}_{suffix.get(col)}\" for i, col in enumerate(cols)]\n",
    "\n",
    "    if isinstance(intervals,list):\n",
    "        assert len(intervals) == len(cols), f\"The fake DataFrame should have {len(cols)} columns but intervals is a list with {len(intervals)} elements\"\n",
    "    else:\n",
    "        if isinstance(intervals,dict):\n",
    "            assert len(set(intervals.keys()) - set(default_intervals.keys())) == 0, f\"The intervals parameter has invalid keys\"\n",
    "            default_intervals.update(intervals)\n",
    "        intervals = [default_intervals[col] for col in cols]\n",
    "    df = pd.DataFrame()\n",
    "    for col, col_name, interval in zip(cols, col_names, intervals):\n",
    "        if interval is None:\n",
    "            interval = default_intervals[col]\n",
    "        assert (len(interval) == 2 and isinstance(interval, tuple)) or isinstance(interval, list), f\"This interval {interval} is neither a tuple of two elements nor a list of strings.\"\n",
    "        if col in (\"i\",\"f\",\"d\"):\n",
    "            start, end = interval\n",
    "        if col == \"i\":\n",
    "            df[col_name] = rng.integers(start, end, size)\n",
    "        elif col == \"f\":\n",
    "            df[col_name] = rng.uniform(start, end, size)\n",
    "        elif col == \"c\":\n",
    "            if isinstance(interval, list):\n",
    "                categories = np.array(interval)\n",
    "            else:\n",
    "                cat_family, length = interval\n",
    "                if isinstance(cat_family, cycle):\n",
    "                    cat_family = next(cat_family)\n",
    "                assert cat_family in categories_dict.keys(), f\"There are no samples for category '{cat_family}'. Consider passing a list of samples or use one of the available categories: {categories_dict.keys()}\"\n",
    "                categories = rng.choice(categories_dict[cat_family], length, replace = False, shuffle = True)\n",
    "            df[col_name] = rng.choice(categories, size, shuffle = True)\n",
    "        elif col == \"d\":\n",
    "            df[col_name] = rng.choice(pd.date_range(start, end), size)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fe4083",
   "metadata": {},
   "source": [
    "__This method is fetched from the article [Generating fake data with pandas, very quickly](https://towardsdatascience.com/generating-fake-data-with-pandas-very-quickly-b99467d4c618)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b20e7039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a dummy data frame with 5000000 rows and 10 columns.\n",
    "# 'c' = category, 'i' = integer, 'f' = float, 'd' = datetime.\n",
    "\n",
    "dummy_df = generate_fake_dataframe(size = 5000000, cols =  \"cififdiccd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ea0ca9ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000000 entries, 0 to 4999999\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Dtype         \n",
      "---  ------          -----         \n",
      " 0   column_0_cat    object        \n",
      " 1   column_1_int    int64         \n",
      " 2   column_2_float  float64       \n",
      " 3   column_3_int    int64         \n",
      " 4   column_4_float  float64       \n",
      " 5   column_5_date   datetime64[ns]\n",
      " 6   column_6_int    int64         \n",
      " 7   column_7_cat    object        \n",
      " 8   column_8_cat    object        \n",
      " 9   column_9_date   datetime64[ns]\n",
      "dtypes: datetime64[ns](2), float64(2), int64(3), object(3)\n",
      "memory usage: 1.2 GB\n"
     ]
    }
   ],
   "source": [
    "# Info about the data frame\n",
    "dummy_df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c54ce5f",
   "metadata": {},
   "source": [
    "As I have mentioned before, every datatype is the largest type as possible. The memory usage is over 1 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "61e01772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the data frame as csv\n",
    "df_as_csv = dummy_df.to_csv('out.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f9cac6",
   "metadata": {},
   "source": [
    "This might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c9c649e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of the csv\n",
    "data_dir = '.\\out.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4c303522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv\n",
    "df = pd.read_csv(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "71f2f5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000000 entries, 0 to 4999999\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Dtype  \n",
      "---  ------          -----  \n",
      " 0   Unnamed: 0      int64  \n",
      " 1   column_0_cat    object \n",
      " 2   column_1_int    int64  \n",
      " 3   column_2_float  float64\n",
      " 4   column_3_int    int64  \n",
      " 5   column_4_float  float64\n",
      " 6   column_5_date   object \n",
      " 7   column_6_int    int64  \n",
      " 8   column_7_cat    object \n",
      " 9   column_8_cat    object \n",
      " 10  column_9_date   object \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 1.8 GB\n"
     ]
    }
   ],
   "source": [
    "# Df info\n",
    "df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5452f9b3",
   "metadata": {},
   "source": [
    "Notice that the datetime columns are now type of object. Now it consumes even more memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9e10aab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index                   128\n",
       "Unnamed: 0         40000000\n",
       "column_0_cat      318994442\n",
       "column_1_int       40000000\n",
       "column_2_float     40000000\n",
       "column_3_int       40000000\n",
       "column_4_float     40000000\n",
       "column_5_date     335000000\n",
       "column_6_int       40000000\n",
       "column_7_cat      308996545\n",
       "column_8_cat      348020080\n",
       "column_9_date     335000000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Memory usage of each column\n",
    "df.memory_usage(index = True, deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0eb393d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall memory consumption: 1798.6404371261597 MB\n"
     ]
    }
   ],
   "source": [
    "# Overall memory consumption\n",
    "memory_consumption = df.memory_usage(index = True, deep = True).sum() / 1024**2\n",
    "print(f\"Overall memory consumption: {memory_consumption} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cce8d11",
   "metadata": {},
   "source": [
    "#### In order to save some memory, we can inspect the columns and alter the datatypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff5b77b",
   "metadata": {},
   "source": [
    " Take the \"column_1_int\" column. It is of type ```int64```. Let's check out the max and min values and see if they \n",
    " span the scope of the datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b5addb56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of column_1_int column is int64\n",
      "Maximum value in column_1_int column is 9\n",
      "Minimum value in column_1_int column is 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Data type of column_1_int column is\", df.column_1_int.dtype)\n",
    "print(\"Maximum value in column_1_int column is\", df.column_1_int.max())\n",
    "print(\"Minimum value in column_1_int column is\", df.column_1_int.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e7513f",
   "metadata": {},
   "source": [
    "As we can see the column has only integers between 0-10 but the datatype is still ```int64```. Now let's cast the dtype to ````int8```` and see the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "51e0825e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing the datatype: 40000128\n",
      "Memory usage after changing the datatype: 5000128\n"
     ]
    }
   ],
   "source": [
    "print(\"Memory usage before changing the datatype:\", df.column_1_int.memory_usage(deep = True))\n",
    "\n",
    "df[\"column_1_int\"] = df.column_1_int.astype(np.int8)\n",
    "\n",
    "print(\"Memory usage after changing the datatype:\", df.column_1_int.memory_usage(deep = True))\n",
    "\n",
    "# It has reduced the memory usage almost by 88%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19020f5d",
   "metadata": {},
   "source": [
    "__It has reduced the memory usage almost by 88%__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9913b594",
   "metadata": {},
   "source": [
    " Take the \"column_2_float\" column. It is of type ``float64``. Let's check out the max and min values and see if they \n",
    " span the scope of the datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1c08d0ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of column_2_float column is float64\n",
      "Maximum value in column_2_float column is 99.99999248133842\n",
      "Minimum value in column_2_float column is 9.635407993702216e-06\n"
     ]
    }
   ],
   "source": [
    "print(\"Data type of column_2_float column is\", df.column_2_float.dtype)\n",
    "print(\"Maximum value in column_2_float column is\", df.column_2_float.max())\n",
    "print(\"Minimum value in column_2_float column is\", df.column_2_float.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4591afc",
   "metadata": {},
   "source": [
    "As we can see the column has only floating point numbers between 0-10 but the datatype is still ```float64```. Now let's cast the dtype to ````float16```` and see the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4d3e6067",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing the datatype: 40000128\n",
      "Memory usage after changing the datatype: 10000128\n"
     ]
    }
   ],
   "source": [
    "print(\"Memory usage before changing the datatype:\", df.column_2_float.memory_usage(deep = True))\n",
    "\n",
    "df[\"column_2_float\"] = df.column_2_float.astype(np.float16)\n",
    "\n",
    "print(\"Memory usage after changing the datatype:\", df.column_2_float.memory_usage(deep = True))\n",
    "\n",
    "# It has reduced the memory usage almost by 75%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6a3391",
   "metadata": {},
   "source": [
    "#### Pandas assign \"object\" for columns with categorical data. \n",
    "#### If we have a column of type \"object\" and has few unique values, we can alter the dtype to \"categorical\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "35ff6bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of column_5_date column is object\n",
      "Number of Unique values in column_5_date column is 366\n",
      "The number of rows 5000000\n"
     ]
    }
   ],
   "source": [
    "# Let's see the number of unique values in 'column_5_date' column\n",
    "\n",
    "print(\"Data type of column_5_date column is\", df.column_5_date.dtype)\n",
    "print(\"Number of Unique values in column_5_date column is\", df.column_5_date.nunique())\n",
    "print(\"The number of rows\", df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d255c7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing the datatype: 40000128\n",
      "Memory usage after changing the datatype: 10011352\n"
     ]
    }
   ],
   "source": [
    "# We have 5000000 values in the 'column_5_date' column but only 366 of them are unique.\n",
    "# It means we can represent this column as 'categorical'\n",
    "\n",
    "print(\"Memory usage before changing the datatype:\", df.column_5_date.memory_usage())\n",
    "\n",
    "df[\"column_5_date\"] = df.column_5_date.astype(\"category\")\n",
    "\n",
    "print(\"Memory usage after changing the datatype:\", df.column_5_date.memory_usage())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8f2a42",
   "metadata": {},
   "source": [
    "#### There are some considerations to keep in mind when representing datetime values as categorical\n",
    "**Loss of precision:** Categorical data type represents data using a numerical code associated with each unique category. By converting datetime values to categorical, you may lose the precision of the original timestamps. If you require high precision for your datetime calculations, it's better to use the 'datetime' data type.\n",
    "\n",
    "**Limited functionality:** Categorical data type provides benefits in terms of memory optimization and faster operations on the column. However, some datetime-specific operations and functions may not be available or may behave differently when applied to categorical datetime values. If your analysis heavily relies on datetime functionality, it's advisable to keep the column as 'datetime' type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c7ce2e",
   "metadata": {},
   "source": [
    "Here is a util that does the downcasting for you. Also when the majority of the values are missing in a column. It keeps that column in a Sparse Array, which saves up some memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "be6cba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARSITY_THRESHOLD = 0.5\n",
    "\n",
    "def optimize(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    float_types = {np.finfo(np.float16).max: np.float16, np.finfo(np.float32).max: np.float32}\n",
    "    int_types = {np.iinfo(np.int8).max: np.int8, np.iinfo(np.int16).max: np.int16, np.iinfo(np.int32).max: np.int32}\n",
    "    \n",
    "    for dtype in ['float64', 'int64']:\n",
    "        selected_columns = df.select_dtypes(include=[dtype])\n",
    "        max_values = selected_columns.max()\n",
    "        types = float_types if dtype == 'float64' else int_types\n",
    "\n",
    "        for col in selected_columns.columns:\n",
    "            if df[col].isna().sum() / len(df[col]) > SPARSITY_THRESHOLD:  # If mostly NaN values\n",
    "                df[col] = pd.arrays.SparseArray(df[col])\n",
    "            else:\n",
    "                max_val = max_values[col]\n",
    "                if not pd.isna(max_val):  # skip columns with only NaNs\n",
    "                    for max_type_val, type_val in types.items():\n",
    "                        if max_val <= max_type_val:\n",
    "                            df[col] = df[col].astype(type_val)\n",
    "                            break\n",
    "\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        if df[col].isna().sum() / len(df[col]) > SPARSITY_THRESHOLD:  # If mostly NaN values\n",
    "            df[col] = pd.arrays.SparseArray(df[col])\n",
    "        else:\n",
    "            try:\n",
    "                df[col] = pd.to_datetime(df[col])\n",
    "            except ValueError:\n",
    "                num_unique_values = len(df[col].unique())\n",
    "                num_total_values = len(df[col])\n",
    "                if num_unique_values / num_total_values < SPARSITY_THRESHOLD:\n",
    "                    df[col] = df[col].astype('category')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1622053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections.abc import Iterable\n",
    "import time\n",
    "\n",
    "SPARSITY_THRESHOLD = 0.5\n",
    "numeric_types = {'float64': {np.finfo(np.float16).max: np.float16,\n",
    "                             np.finfo(np.float32).max: np.float32},\n",
    "                 'int64': {np.iinfo(np.int8).max: np.int8,\n",
    "                           np.iinfo(np.int16).max: np.int16,\n",
    "                           np.iinfo(np.int32).max: np.int32}}\n",
    "\n",
    "def downcast_numeric(series):\n",
    "    max_val = series.max()\n",
    "    if pd.notna(max_val):\n",
    "        for max_type_val, type_val in numeric_types[series.dtype.name].items():\n",
    "            if max_val <= max_type_val:\n",
    "                return series.astype(type_val)\n",
    "    return series\n",
    "\n",
    "def convert_object(series):\n",
    "    # Try to convert the series to numeric\n",
    "    converted_series = pd.to_numeric(series, errors='coerce')\n",
    "\n",
    "    # If the series does not contain more np.nan after conversion, the conversion was successful\n",
    "    if series.isna().sum() >= converted_series.isna().sum():\n",
    "        series = downcast_numeric(converted_series)\n",
    "    else:\n",
    "        try:\n",
    "            series = pd.to_datetime(series)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        if len(series.unique()) / len(series) < SPARSITY_THRESHOLD:\n",
    "            series = series.astype('category')\n",
    "        else:\n",
    "            series = series.astype('string')\n",
    "\n",
    "    return series\n",
    "\n",
    "\n",
    "def optimize(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Minimizes memory usage by using smaller dtypes\n",
    "    :param df: dataframe input\n",
    "    :return: optimized df\n",
    "    \"\"\"\n",
    "\n",
    "    total_start = time.time()\n",
    "\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_series = df[col]\n",
    "        col_dtype = col_series.dtype.name\n",
    "        \n",
    "        if col_series.isna().all():\n",
    "            continue\n",
    "        \n",
    "        if col_series.isna().mean() > SPARSITY_THRESHOLD:\n",
    "            df[col] = pd.arrays.SparseArray(col_series, dtype=col_dtype)\n",
    "        elif col_dtype == 'object' and not any(isinstance(val, Iterable) and not isinstance(val, str) for val in col_series.dropna()):\n",
    "            df[col] = convert_object(col_series)\n",
    "        elif col_dtype in numeric_types:\n",
    "            df[col] = downcast_numeric(col_series)\n",
    "        elif pd.api.types.is_sparse(col_series.dtype) and col_series.notna().mean() > 0.5:\n",
    "            df[col] = col_series.to_dense()\n",
    "        \n",
    "    print(f\"Total execution time was {time.time() - total_start} seconds\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777b9e8c",
   "metadata": {},
   "source": [
    "## How does it work?\n",
    "\n",
    "### A. optimize(df: pd.DataFrame):\n",
    "1. If df is empty __&rarr;__ ``return df``\n",
    "2. If current column is all NaN __&rarr;__ ``continue``\n",
    "3. If more than half is NaN __&rarr;__ Convert to ``SparseArray``\n",
    "4. If dtype is ``object`` && not contains ``Iterable``(list, tuple, set) except for ``str`` __&rarr;__ ``convert_object(current_column)``\n",
    "5. If dtype is numeric __&rarr;__ ``downcast_numeric(current_column)``\n",
    "6. If dtype is Sparse && more than half is non NaN __&rarr;__ Covert it back to ``DenseArray``\n",
    "\n",
    "### B. convert_object(series):\n",
    "1. Try to convert the column into numeric data type.\n",
    "2. If conversion successfull __&rarr;__ ``downcast_numeric(current_column)``.\n",
    "3. Else __&rarr;__ try to convert to ``datetime64[ns]``.\n",
    "4. If neither numeric nor datetime works && If #unique_values < ``SPARSITY_THRESHOLD``, convert to ``category`` \n",
    "5. Else __&rarr;__ convert to ``string``\n",
    "\n",
    "### C. downcast_numeric(series):\n",
    "1. If ``max_val`` is __not__ null __&rarr;__ Downgrade to the smallest possible numeric type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5905d532",
   "metadata": {},
   "source": [
    "__Here is a demonstration of the ``optimize()`` method__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "6fcff661",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000000 entries, 0 to 4999999\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Dtype  \n",
      "---  ------          -----  \n",
      " 0   Unnamed: 0      int64  \n",
      " 1   column_0_cat    object \n",
      " 2   column_1_int    int64  \n",
      " 3   column_2_float  float64\n",
      " 4   column_3_int    int64  \n",
      " 5   column_4_float  float64\n",
      " 6   column_5_date   object \n",
      " 7   column_6_int    int64  \n",
      " 8   column_7_cat    object \n",
      " 9   column_8_cat    object \n",
      " 10  column_9_date   object \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 1.8 GB\n",
      "None\n",
      "***********************\n",
      "Memory usage of df before optimization: 1798.64 MB\n",
      "**************************************************\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000000 entries, 0 to 4999999\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Dtype         \n",
      "---  ------          -----         \n",
      " 0   Unnamed: 0      int32         \n",
      " 1   column_0_cat    category      \n",
      " 2   column_1_int    int8          \n",
      " 3   column_2_float  float16       \n",
      " 4   column_3_int    int8          \n",
      " 5   column_4_float  float16       \n",
      " 6   column_5_date   datetime64[ns]\n",
      " 7   column_6_int    int8          \n",
      " 8   column_7_cat    category      \n",
      " 9   column_8_cat    category      \n",
      " 10  column_9_date   datetime64[ns]\n",
      "dtypes: category(3), datetime64[ns](2), float16(2), int32(1), int8(3)\n",
      "memory usage: 143.1 MB\n",
      "None\n",
      "***********************\n",
      "Memory usage of df before optimization: 143.05 MB\n"
     ]
    }
   ],
   "source": [
    "# Trying out the optimize() method\n",
    "\n",
    "df_temp = pd.read_csv(data_dir)\n",
    "print(df_temp.info(memory_usage = \"deep\"))\n",
    "print(\"***********************\")\n",
    "print(\"Memory usage of df before optimization: {:.2f} MB\".format(df_temp.memory_usage(deep=True).sum() / 1024**2))\n",
    "\n",
    "print(\"**************************************************\")\n",
    "\n",
    "df_temp = optimize(df_temp)\n",
    "print(df_temp.info(memory_usage = \"deep\"))\n",
    "print(\"***********************\")\n",
    "print(\"Memory usage of df before optimization: {:.2f} MB\".format(df_temp.memory_usage(deep=True).sum() / 1024**2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ab3d0f",
   "metadata": {},
   "source": [
    "__Simulating a case:__ Merging the optimized dataframe with another dataframe that has an integer value larger than ``int8`` under a column with the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8af0f572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   column_0_cat    5000 non-null   object        \n",
      " 1   column_1_int    5000 non-null   int64         \n",
      " 2   column_2_float  5000 non-null   float64       \n",
      " 3   column_3_int    5000 non-null   int64         \n",
      " 4   column_4_float  5000 non-null   float64       \n",
      " 5   column_5_date   5000 non-null   datetime64[ns]\n",
      " 6   column_6_int    5000 non-null   int64         \n",
      " 7   column_7_cat    5000 non-null   object        \n",
      " 8   column_8_cat    5000 non-null   object        \n",
      " 9   column_9_date   5000 non-null   datetime64[ns]\n",
      "dtypes: datetime64[ns](2), float64(2), int64(3), object(3)\n",
      "memory usage: 390.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Generating a dummy data frame with 5000 rows and 10 columns. Merging two dataframes with 5 million rows will exceed the memory\n",
    "# constraints.\n",
    "\n",
    "dummy_df_2 = generate_fake_dataframe(size = 5000, cols =  \"cififdiccd\")\n",
    "df_optimized = dummy_df_2\n",
    "dummy_df_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "06110f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   column_0_cat    5000 non-null   category      \n",
      " 1   column_1_int    5000 non-null   int32         \n",
      " 2   column_2_float  5000 non-null   float16       \n",
      " 3   column_3_int    5000 non-null   int8          \n",
      " 4   column_4_float  5000 non-null   float16       \n",
      " 5   column_5_date   5000 non-null   datetime64[ns]\n",
      " 6   column_6_int    5000 non-null   int8          \n",
      " 7   column_7_cat    5000 non-null   category      \n",
      " 8   column_8_cat    5000 non-null   category      \n",
      " 9   column_9_date   5000 non-null   datetime64[ns]\n",
      "dtypes: category(3), datetime64[ns](2), float16(2), int32(1), int8(2)\n",
      "memory usage: 142.3 KB\n"
     ]
    }
   ],
   "source": [
    "df_optimized = optimize(df_optimized)\n",
    "df_optimized.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "8a9ba96b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_0_cat</th>\n",
       "      <th>column_1_int</th>\n",
       "      <th>column_2_float</th>\n",
       "      <th>column_3_int</th>\n",
       "      <th>column_4_float</th>\n",
       "      <th>column_5_date</th>\n",
       "      <th>column_6_int</th>\n",
       "      <th>column_7_cat</th>\n",
       "      <th>column_8_cat</th>\n",
       "      <th>column_9_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Patricia</td>\n",
       "      <td>4</td>\n",
       "      <td>24.7500</td>\n",
       "      <td>4</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>2020-08-05</td>\n",
       "      <td>1</td>\n",
       "      <td>horse</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>2020-04-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Michael</td>\n",
       "      <td>0</td>\n",
       "      <td>79.0625</td>\n",
       "      <td>9</td>\n",
       "      <td>95.125000</td>\n",
       "      <td>2020-04-04</td>\n",
       "      <td>5</td>\n",
       "      <td>pig</td>\n",
       "      <td>Nairobi</td>\n",
       "      <td>2020-08-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Michael</td>\n",
       "      <td>10000000</td>\n",
       "      <td>78.4375</td>\n",
       "      <td>6</td>\n",
       "      <td>1.405273</td>\n",
       "      <td>2020-11-13</td>\n",
       "      <td>8</td>\n",
       "      <td>pig</td>\n",
       "      <td>Lisbon</td>\n",
       "      <td>2020-02-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Susan</td>\n",
       "      <td>4</td>\n",
       "      <td>79.3125</td>\n",
       "      <td>1</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>2020-09-10</td>\n",
       "      <td>7</td>\n",
       "      <td>cow</td>\n",
       "      <td>Bogotá</td>\n",
       "      <td>2020-06-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Michael</td>\n",
       "      <td>4</td>\n",
       "      <td>85.7500</td>\n",
       "      <td>9</td>\n",
       "      <td>72.562500</td>\n",
       "      <td>2020-06-13</td>\n",
       "      <td>4</td>\n",
       "      <td>fish</td>\n",
       "      <td>Bogotá</td>\n",
       "      <td>2020-02-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  column_0_cat  column_1_int  column_2_float  column_3_int  column_4_float  \\\n",
       "0     Patricia             4         24.7500             4       57.000000   \n",
       "1      Michael             0         79.0625             9       95.125000   \n",
       "2      Michael      10000000         78.4375             6        1.405273   \n",
       "3        Susan             4         79.3125             1       85.000000   \n",
       "4      Michael             4         85.7500             9       72.562500   \n",
       "\n",
       "  column_5_date  column_6_int column_7_cat column_8_cat column_9_date  \n",
       "0    2020-08-05             1        horse    Marseille    2020-04-27  \n",
       "1    2020-04-04             5          pig      Nairobi    2020-08-19  \n",
       "2    2020-11-13             8          pig       Lisbon    2020-02-09  \n",
       "3    2020-09-10             7          cow       Bogotá    2020-06-22  \n",
       "4    2020-06-13             4         fish       Bogotá    2020-02-03  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inserting a large integer number\n",
    "\n",
    "df_to_merge = dummy_df_2\n",
    "df_to_merge.at[2, 'column_1_int'] = 10000000\n",
    "df_to_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "807636b8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Performing a left join\n",
    "merged_df = pd.merge(df_to_merge, df_optimized, on='column_1_int', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "012ae6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2502224 entries, 0 to 2502223\n",
      "Data columns (total 19 columns):\n",
      " #   Column            Dtype         \n",
      "---  ------            -----         \n",
      " 0   column_0_cat_x    category      \n",
      " 1   column_1_int      int32         \n",
      " 2   column_2_float_x  float16       \n",
      " 3   column_3_int_x    int8          \n",
      " 4   column_4_float_x  float16       \n",
      " 5   column_5_date_x   datetime64[ns]\n",
      " 6   column_6_int_x    int8          \n",
      " 7   column_7_cat_x    category      \n",
      " 8   column_8_cat_x    category      \n",
      " 9   column_9_date_x   datetime64[ns]\n",
      " 10  column_0_cat_y    category      \n",
      " 11  column_2_float_y  float16       \n",
      " 12  column_3_int_y    int8          \n",
      " 13  column_4_float_y  float16       \n",
      " 14  column_5_date_y   datetime64[ns]\n",
      " 15  column_6_int_y    int8          \n",
      " 16  column_7_cat_y    category      \n",
      " 17  column_8_cat_y    category      \n",
      " 18  column_9_date_y   datetime64[ns]\n",
      "dtypes: category(6), datetime64[ns](4), float16(4), int32(1), int8(4)\n",
      "memory usage: 148.0 MB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info(memory_usage = \"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1592a6",
   "metadata": {},
   "source": [
    "As we can see the datatypes are back to being unefficient. Optimizing again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "746194bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = optimize(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b63a52f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2502224 entries, 0 to 2502223\n",
      "Data columns (total 19 columns):\n",
      " #   Column            Dtype         \n",
      "---  ------            -----         \n",
      " 0   column_0_cat_x    category      \n",
      " 1   column_1_int      int32         \n",
      " 2   column_2_float_x  float16       \n",
      " 3   column_3_int_x    int8          \n",
      " 4   column_4_float_x  float16       \n",
      " 5   column_5_date_x   datetime64[ns]\n",
      " 6   column_6_int_x    int8          \n",
      " 7   column_7_cat_x    category      \n",
      " 8   column_8_cat_x    category      \n",
      " 9   column_9_date_x   datetime64[ns]\n",
      " 10  column_0_cat_y    category      \n",
      " 11  column_2_float_y  float16       \n",
      " 12  column_3_int_y    int8          \n",
      " 13  column_4_float_y  float16       \n",
      " 14  column_5_date_y   datetime64[ns]\n",
      " 15  column_6_int_y    int8          \n",
      " 16  column_7_cat_y    category      \n",
      " 17  column_8_cat_y    category      \n",
      " 18  column_9_date_y   datetime64[ns]\n",
      "dtypes: category(6), datetime64[ns](4), float16(4), int32(1), int8(4)\n",
      "memory usage: 148.0 MB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info(memory_usage = \"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf73f581",
   "metadata": {},
   "source": [
    "### Using pandas' ``StringDtype`` instead of ``object`` for string columns\n",
    "\n",
    "When handling ``string`` data, ``object`` dtype in ``Pandas`` is a flexible type, because it can hold mixed types, like numbers or strings. However, this comes with a trade-off: it requires more memory and computation time, because every element is a Python object, which needs to store extra information like the type info, reference count, etc. The ``StringDtype``, on the other hand, is specifically for ``string`` data. This specificity leads to more efficient memory usage and faster computations, as it can leverage vectorized operations of ``NumPy`` and ``Pandas``.\n",
    "\n",
    "Let's see make a comparison:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960c0cfa",
   "metadata": {},
   "source": [
    "First, two dummy dataframes for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5be597f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5_000_000  # Number of rows\n",
    "\n",
    "# Create a datetime column with random dates within a range\n",
    "start_date = pd.to_datetime('2000-01-01')\n",
    "end_date = pd.to_datetime('2020-12-31')\n",
    "date_range = (end_date - start_date).days\n",
    "date_column = start_date + pd.to_timedelta(np.random.randint(0, date_range, n), unit='d')\n",
    "\n",
    "# Create a column with numeric values as strings\n",
    "numeric_string_column = np.random.randint(0, 1000000, n).astype(str)\n",
    "\n",
    "# Create an integer column\n",
    "integer_column = np.random.randint(0, 1000000, n)\n",
    "\n",
    "# Create a float column\n",
    "float_column = np.random.rand(n)\n",
    "\n",
    "# Create a NaN column\n",
    "nan_column = np.nan\n",
    "\n",
    "# Create a categorical column\n",
    "categories = ['cat', 'dog', 'mouse', 'fish', 'bird']\n",
    "categorical_column = (np.random.choice(categories, n))\n",
    "\n",
    "# Create a string column\n",
    "string_column = ['string' + str(i) for i in np.random.randint(0, n*2, n)]\n",
    "\n",
    "# Construct the DataFrame\n",
    "df_test_str = pd.DataFrame({\n",
    "    'Date': date_column,\n",
    "    'NumericString': numeric_string_column,\n",
    "    'Integer': integer_column,\n",
    "    'Float': float_column,\n",
    "    'Category': categorical_column,\n",
    "    'NaN': nan_column,\n",
    "    'String': string_column\n",
    "})\n",
    "\n",
    "df_test_obj = pd.DataFrame({\n",
    "    'Date': date_column,\n",
    "    'NumericString': numeric_string_column,\n",
    "    'Integer': integer_column,\n",
    "    'Float': float_column,\n",
    "    'Category': categorical_column,\n",
    "    'NaN': nan_column,\n",
    "    'String': string_column\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e34bea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000000 entries, 0 to 4999999\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Dtype         \n",
      "---  ------         -----         \n",
      " 0   Date           datetime64[ns]\n",
      " 1   NumericString  object        \n",
      " 2   Integer        int32         \n",
      " 3   Float          float64       \n",
      " 4   Category       object        \n",
      " 5   NaN            float64       \n",
      " 6   String         object        \n",
      "dtypes: datetime64[ns](1), float64(2), int32(1), object(3)\n",
      "memory usage: 1.0 GB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000000 entries, 0 to 4999999\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Dtype         \n",
      "---  ------         -----         \n",
      " 0   Date           datetime64[ns]\n",
      " 1   NumericString  object        \n",
      " 2   Integer        int32         \n",
      " 3   Float          float64       \n",
      " 4   Category       object        \n",
      " 5   NaN            float64       \n",
      " 6   String         object        \n",
      "dtypes: datetime64[ns](1), float64(2), int32(1), object(3)\n",
      "memory usage: 1.0 GB\n"
     ]
    }
   ],
   "source": [
    "df_test_str.info(memory_usage=\"deep\")\n",
    "df_test_obj.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eabbffb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>NumericString</th>\n",
       "      <th>Integer</th>\n",
       "      <th>Float</th>\n",
       "      <th>Category</th>\n",
       "      <th>NaN</th>\n",
       "      <th>String</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2005-06-25</td>\n",
       "      <td>3218</td>\n",
       "      <td>701295</td>\n",
       "      <td>0.297821</td>\n",
       "      <td>cat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>string9148254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-07-06</td>\n",
       "      <td>899628</td>\n",
       "      <td>327548</td>\n",
       "      <td>0.543676</td>\n",
       "      <td>bird</td>\n",
       "      <td>NaN</td>\n",
       "      <td>string311467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-06-30</td>\n",
       "      <td>412683</td>\n",
       "      <td>403865</td>\n",
       "      <td>0.234883</td>\n",
       "      <td>mouse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>string570901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-02-08</td>\n",
       "      <td>585943</td>\n",
       "      <td>213214</td>\n",
       "      <td>0.305303</td>\n",
       "      <td>dog</td>\n",
       "      <td>NaN</td>\n",
       "      <td>string8689725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-11-20</td>\n",
       "      <td>134002</td>\n",
       "      <td>7450</td>\n",
       "      <td>0.114997</td>\n",
       "      <td>bird</td>\n",
       "      <td>NaN</td>\n",
       "      <td>string4009548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date NumericString  Integer     Float Category  NaN         String\n",
       "0 2005-06-25          3218   701295  0.297821      cat  NaN  string9148254\n",
       "1 2013-07-06        899628   327548  0.543676     bird  NaN   string311467\n",
       "2 2009-06-30        412683   403865  0.234883    mouse  NaN   string570901\n",
       "3 2020-02-08        585943   213214  0.305303      dog  NaN  string8689725\n",
       "4 2009-11-20        134002     7450  0.114997     bird  NaN  string4009548"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_str.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "268e514c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of 'String' column in df_test_obj: 333.2575054168701 MB\n",
      "Memory usage of 'String' column in df_test_str: 333.2575054168701 MB\n",
      "Time taken for filtering operation on df_test_obj: 0.21980786323547363 seconds\n",
      "Time taken for filtering operation on df_test_str: 0.2734074592590332 seconds\n",
      "Time taken for comparison operation on df_test_obj: 0.19751667976379395 seconds\n",
      "Time taken for comparison operation on df_test_str: 0.2770423889160156 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Convert the 'String' column in df_test_str to StringDtype\n",
    "# 'string' is an alias for StringDtype.\n",
    "df_test_str['String'] = df_test_str['String'].astype('string')\n",
    "\n",
    "# Compare memory usage\n",
    "print(f\"Memory usage of 'String' column in df_test_obj: {df_test_obj['String'].memory_usage(deep=True) / 1024**2} MB\")\n",
    "print(f\"Memory usage of 'String' column in df_test_str: {df_test_str['String'].memory_usage(deep=True) / 1024**2} MB\")\n",
    "\n",
    "# Perform filtering operation and compare execution time\n",
    "start_time = time.time()\n",
    "df_test_obj[df_test_obj['String'] == 'string5000000']\n",
    "print(f\"Time taken for filtering operation on df_test_obj: {time.time() - start_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "df_test_str[df_test_str['String'] == 'string5000000']\n",
    "print(f\"Time taken for filtering operation on df_test_str: {time.time() - start_time} seconds\")\n",
    "\n",
    "# Perform comparison operation and compare execution time\n",
    "start_time = time.time()\n",
    "df_test_obj['String'] == 'string5000000'\n",
    "print(f\"Time taken for comparison operation on df_test_obj: {time.time() - start_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "df_test_str['String'] == 'string5000000'\n",
    "print(f\"Time taken for comparison operation on df_test_str: {time.time() - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0172f8",
   "metadata": {},
   "source": [
    "After all that explanation we would have expected to see the ``StringDtype`` to use less memory and be faster. However, it's important to consider that the performance of different data types can vary based on a number of factors, including the specific operation being performed, the size and distribution of the data, etc.\n",
    "\n",
    "In this case, when the ``StringDtype`` is used, Pandas actually uses an array of pointers to Python ``string`` objects behind the scenes. These ``objects`` are the same as those used in the object dtype, so the memory usage ends up being roughly the same. Additionally, because the Python strings are immutable, certain operations like filtering or comparing might result in creating new Python objects, which could make these operations slower.\n",
    "\n",
    "Let's try something else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0218532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of 'String' column in df_test_obj: 333.2575054168701 MB\n",
      "Memory usage of 'String' column in df_test_str: 80.53381156921387 MB\n",
      "Time taken for filtering operation on df_test_obj: 0.2047138214111328 seconds\n",
      "Time taken for filtering operation on df_test_str: 0.04249691963195801 seconds\n",
      "Time taken for comparison operation on df_test_obj: 0.21765780448913574 seconds\n",
      "Time taken for comparison operation on df_test_str: 0.04292774200439453 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pyarrow\n",
    "\n",
    "# Convert the 'String' column in df_test_str to StringDtype\n",
    "# 'string' is an alias for StringDtype.\n",
    "df_test_str['String'] = df_test_str['String'].astype(pd.StringDtype(storage='pyarrow'))\n",
    "\n",
    "# Compare memory usage\n",
    "print(f\"Memory usage of 'String' column in df_test_obj: {df_test_obj['String'].memory_usage(deep=True) / 1024**2} MB\")\n",
    "print(f\"Memory usage of 'String' column in df_test_str: {df_test_str['String'].memory_usage(deep=True) / 1024**2} MB\")\n",
    "\n",
    "# Perform filtering operation and compare execution time\n",
    "start_time = time.time()\n",
    "df_test_obj[df_test_obj['String'] == 'string5000000']\n",
    "print(f\"Time taken for filtering operation on df_test_obj: {time.time() - start_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "df_test_str[df_test_str['String'] == 'string5000000']\n",
    "print(f\"Time taken for filtering operation on df_test_str: {time.time() - start_time} seconds\")\n",
    "\n",
    "# Perform comparison operation and compare execution time\n",
    "start_time = time.time()\n",
    "df_test_obj['String'] == 'string5000000'\n",
    "print(f\"Time taken for comparison operation on df_test_obj: {time.time() - start_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "df_test_str['String'] == 'string5000000'\n",
    "print(f\"Time taken for comparison operation on df_test_str: {time.time() - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d6b6f6",
   "metadata": {},
   "source": [
    "When we specify ``storage='pyarrow'`` while using ``pd.StringDtype()``, we are opting to use Apache Arrow's efficient storage format for strings. In this format, strings are stored in a contiguous block of memory (a buffer) as opposed to being stored as separate Python string objects.\n",
    "\n",
    "Each Python ``string`` object has a fixed overhead in terms of memory usage, and if we have a large number of small strings, this overhead can be significant. In contrast, Apache Arrow's storage format has a lower memory overhead, leading to the observed reduction in memory usage when using ``pd.StringDtype(storage='pyarrow')``.\n",
    "\n",
    "Moreover, Apache Arrow uses an optimized, vectorized data processing library that can be significantly faster than the default Python methods for operations on strings. This is why we see a speedup for the filtering and comparison operations when we use ``pd.StringDtype(storage='pyarrow')``.\n",
    "\n",
    "As a result, if you have a large amount of string data and performance is a concern, pd.StringDtype(storage='pyarrow') is likely the best choice. Note that, it requires the ``PyArrow`` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3902eab2",
   "metadata": {},
   "source": [
    "Another thing, there is a configuration optin in pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "643e2942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n"
     ]
    }
   ],
   "source": [
    "print(pd.options.mode.string_storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45202492",
   "metadata": {},
   "source": [
    " It controls how string data types are stored internally in a DataFrame. By default, it is set to ``python``, indicating that the Python's built-in str data type is used to store string data. In this mode, string data is stored as an object data type (object) in pandas, which can consume a lot of memory for large datasets. So when we use ``astype('string')``, it is equal to ``astype(pd.StringDtype(storage='python'))``.\n",
    " \n",
    " We can reconfigure it to use ``pyarrow`` instead by setting ``print(pd.options.mode.string_storage)`` to ``pyarrow``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "eb0d5e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow\n"
     ]
    }
   ],
   "source": [
    "pd.options.mode.string_storage = 'pyarrow'\n",
    "print(pd.options.mode.string_storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55b8622",
   "metadata": {},
   "source": [
    "It indicates that the PyArrow library's ``StringArray`` type is to be used to store string data. Now when we use ``astype('string')``, it is equal to ``astype(pd.StringDtype(storage='pyarrow'))``. Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "00ca3ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of 'String' column in df_test_obj: 333.2575054168701 MB\n",
      "Memory usage of 'String' column in df_test_str: 80.53381156921387 MB\n",
      "Time taken for filtering operation on df_test_obj: 0.20021653175354004 seconds\n",
      "Time taken for filtering operation on df_test_str: 0.060353994369506836 seconds\n",
      "Time taken for comparison operation on df_test_obj: 0.19650554656982422 seconds\n",
      "Time taken for comparison operation on df_test_str: 0.04240989685058594 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pyarrow\n",
    "\n",
    "# Convert the 'String' column in df_test_str to StringDtype\n",
    "# 'string' is an alias for StringDtype.\n",
    "df_test_str['String'] = df_test_str['String'].astype('string')\n",
    "\n",
    "# Compare memory usage\n",
    "print(f\"Memory usage of 'String' column in df_test_obj: {df_test_obj['String'].memory_usage(deep=True) / 1024**2} MB\")\n",
    "print(f\"Memory usage of 'String' column in df_test_str: {df_test_str['String'].memory_usage(deep=True) / 1024**2} MB\")\n",
    "\n",
    "# Perform filtering operation and compare execution time\n",
    "start_time = time.time()\n",
    "df_test_obj[df_test_obj['String'] == 'string5000000']\n",
    "print(f\"Time taken for filtering operation on df_test_obj: {time.time() - start_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "df_test_str[df_test_str['String'] == 'string5000000']\n",
    "print(f\"Time taken for filtering operation on df_test_str: {time.time() - start_time} seconds\")\n",
    "\n",
    "# Perform comparison operation and compare execution time\n",
    "start_time = time.time()\n",
    "df_test_obj['String'] == 'string5000000'\n",
    "print(f\"Time taken for comparison operation on df_test_obj: {time.time() - start_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "df_test_str['String'] == 'string5000000'\n",
    "print(f\"Time taken for comparison operation on df_test_str: {time.time() - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1174ca2f",
   "metadata": {},
   "source": [
    "### Considerations for Specific Scenarios:\n",
    "\n",
    "#### 1- Manipulation on ``category`` type columns is tricky[^1]:\n",
    "- __When operating on categorical columns,__ select options which operate on the categories in the datatype rather than the values in the series which contain the datatype. This should allow you to preserve the categorical nature and also improve performance.\n",
    "- __When merging on categorical columns,__ be aware that to preserve the categorical nature, the categorical types in the merge columns of each dataframe must match exactly.\n",
    "- __When grouping on categorical columns,__ by default you will get a result for each value in the datatype, even if it’s not present in the data, you can change this using observed=True in the .groupby .\n",
    "- __When things that you expect to work unexpectedly stop working,__ consider whether a strange interaction with categoricals may be at play\n",
    "\n",
    "[^1]:https://towardsdatascience.com/staying-sane-while-adopting-pandas-categorical-datatypes-78dbd19dcd8a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5552b65",
   "metadata": {},
   "source": [
    " Since there are a number of considerations we need to be aware of when working with ``category`` columns, it might be difficult to start using the ``optimize`` method in an existing project where there are lots of manipulation going on. Therefore, here is a version of the ``optimize`` method which doesn't convert to ``category``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb63324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections.abc import Iterable\n",
    "import time\n",
    "\n",
    "SPARSITY_THRESHOLD = 0.5\n",
    "numeric_types = {'float64': {np.finfo(np.float16).max: np.float16,\n",
    "                             np.finfo(np.float32).max: np.float32},\n",
    "                 'int64': {np.iinfo(np.int8).max: np.int8,\n",
    "                           np.iinfo(np.int16).max: np.int16,\n",
    "                           np.iinfo(np.int32).max: np.int32}}\n",
    "\n",
    "def downcast_numeric(series):\n",
    "    max_val = series.max()\n",
    "    if pd.notna(max_val):\n",
    "        for max_type_val, type_val in numeric_types[series.dtype.name].items():\n",
    "            if max_val <= max_type_val:\n",
    "                return series.astype(type_val)\n",
    "    return series\n",
    "\n",
    "def convert_object(series):\n",
    "    # Try to convert the series to numeric\n",
    "    converted_series = pd.to_numeric(series, errors='coerce')\n",
    "\n",
    "    # If the series does not contain more np.nan after conversion, the conversion was successful\n",
    "    if series.isna().sum() >= converted_series.isna().sum():\n",
    "        series = downcast_numeric(converted_series)\n",
    "    else:\n",
    "        try:\n",
    "            series = pd.to_datetime(series)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        if len(series.unique()) / len(series) < SPARSITY_THRESHOLD:\n",
    "            series = series.astype('category')\n",
    "        else:\n",
    "            series = series.astype('string')\n",
    "\n",
    "    return series\n",
    "\n",
    "\n",
    "def optimize(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Minimizes memory usage by using smaller dtypes\n",
    "    :param df: dataframe input\n",
    "    :return: optimized df\n",
    "    \"\"\"\n",
    "\n",
    "    total_start = time.time()\n",
    "\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_series = df[col]\n",
    "        col_dtype = col_series.dtype.name\n",
    "        \n",
    "        if col_series.isna().all():\n",
    "            continue\n",
    "        \n",
    "        if col_series.isna().mean() > SPARSITY_THRESHOLD:\n",
    "            df[col] = pd.arrays.SparseArray(col_series, dtype=col_dtype)\n",
    "        elif col_dtype == 'object' and not any(isinstance(val, Iterable) and not isinstance(val, str) for val in col_series.dropna()):\n",
    "            df[col] = convert_object(col_series)\n",
    "        elif col_dtype in numeric_types:\n",
    "            df[col] = downcast_numeric(col_series)\n",
    "        elif pd.api.types.is_sparse(col_series.dtype) and col_series.notna().mean() > 0.5:\n",
    "            df[col] = col_series.to_dense()\n",
    "        \n",
    "    print(f\"Total execution time was {time.time() - total_start} seconds\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e50f2a",
   "metadata": {},
   "source": [
    "#### 2- Working with ``SparseArray``:\n",
    "- Trying out ``merge`` and ``concat``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2eb0f6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype               \n",
      "---  ------  --------------  -----               \n",
      " 0   A       5 non-null      Sparse[int64, 0]    \n",
      " 1   B       2 non-null      Sparse[object, nan] \n",
      " 2   C       1 non-null      Sparse[float64, nan]\n",
      " 3   D       2 non-null      Sparse[object, nan] \n",
      "dtypes: Sparse[float64, nan](1), Sparse[int64, 0](1), Sparse[object, nan](2)\n",
      "memory usage: 212.0 bytes\n"
     ]
    }
   ],
   "source": [
    "# Two dummy dataframes to simulate\n",
    "data = {\n",
    "    'A': [1, 0, 0, 0, 5],\n",
    "    'B': ['x', np.nan, np.nan, np.nan, 'e'],\n",
    "    'C': [1.23, np.nan, np.nan, np.nan, np.nan],\n",
    "    'D': ['y', np.nan, np.nan, np.nan, 'z']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df['A'] = pd.arrays.SparseArray(df['A'], dtype='int64')\n",
    "df['B'] = pd.arrays.SparseArray(df['B'], dtype='object')\n",
    "df['C'] = pd.arrays.SparseArray(df['C'], dtype='float64')\n",
    "df['D'] = pd.arrays.SparseArray(df['D'], dtype='object')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6773745b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   A       5 non-null      int64  \n",
      " 1   B       2 non-null      string \n",
      " 2   C       1 non-null      float64\n",
      " 3   D       2 non-null      object \n",
      "dtypes: float64(1), int64(1), object(1), string(1)\n",
      "memory usage: 288.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "data2 = {\n",
    "    'A': [1, 4, 7, 8, 5],\n",
    "    'B': ['x', np.nan, np.nan, np.nan, 'e'],\n",
    "    'C': [1.23, np.nan, np.nan, np.nan, np.nan],\n",
    "    'D': ['y', np.nan, np.nan, np.nan, 'z']\n",
    "}\n",
    "\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "df2['B'] = df2['B'].astype('string')\n",
    "\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2b978be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_sparse = df.merge(df2, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8ed1b531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5 entries, 0 to 4\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype               \n",
      "---  ------  --------------  -----               \n",
      " 0   A       5 non-null      Sparse[int64, 0]    \n",
      " 1   B       2 non-null      Sparse[object, nan] \n",
      " 2   C       1 non-null      Sparse[float64, nan]\n",
      " 3   D       2 non-null      Sparse[object, nan] \n",
      "dtypes: Sparse[float64, nan](1), Sparse[int64, 0](1), Sparse[object, nan](2)\n",
      "memory usage: 124.0 bytes\n"
     ]
    }
   ],
   "source": [
    "merge_sparse.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "66429e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>x</td>\n",
       "      <td>1.23</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A    B     C    D\n",
       "0  1    x  1.23    y\n",
       "1  0  NaN   NaN  NaN\n",
       "2  0  NaN   NaN  NaN\n",
       "3  0  NaN   NaN  NaN\n",
       "4  5    e   NaN    z"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6194e4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_sparse = pd.concat([df, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "088da850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10 entries, 0 to 4\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype               \n",
      "---  ------  --------------  -----               \n",
      " 0   A       10 non-null     Sparse[int64, 0]    \n",
      " 1   B       4 non-null      object              \n",
      " 2   C       2 non-null      Sparse[float64, nan]\n",
      " 3   D       4 non-null      Sparse[object, nan] \n",
      "dtypes: Sparse[float64, nan](1), Sparse[int64, 0](1), Sparse[object, nan](1), object(1)\n",
      "memory usage: 316.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "concat_sparse.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4fe97435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>x</td>\n",
       "      <td>1.23</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>x</td>\n",
       "      <td>1.23</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A     B     C    D\n",
       "0  1     x  1.23    y\n",
       "1  0   NaN   NaN  NaN\n",
       "2  0   NaN   NaN  NaN\n",
       "3  0   NaN   NaN  NaN\n",
       "4  5     e   NaN    z\n",
       "0  1     x  1.23    y\n",
       "1  4  <NA>   NaN  NaN\n",
       "2  7  <NA>   NaN  NaN\n",
       "3  8  <NA>   NaN  NaN\n",
       "4  5     e   NaN    z"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49af1195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   A       4 non-null      object\n",
      " 1   B       4 non-null      object\n",
      " 2   C       0 non-null      object\n",
      " 3   D       5 non-null      int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 288.0+ bytes\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   A       4 non-null      object\n",
      " 1   B       4 non-null      object\n",
      " 2   C       5 non-null      int64 \n",
      " 3   D       5 non-null      int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 288.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()\n",
    "df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e074f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total execution time was 0.002992391586303711 seconds\n",
      "Total execution time was 0.0032334327697753906 seconds\n"
     ]
    }
   ],
   "source": [
    "df = optimize(df)\n",
    "df3 = optimize(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63563853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   A       4 non-null      float16\n",
      " 1   B       4 non-null      string \n",
      " 2   C       0 non-null      object \n",
      " 3   D       5 non-null      int8   \n",
      "dtypes: float16(1), int8(1), object(1), string(1)\n",
      "memory usage: 223.0+ bytes\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   A       4 non-null      float16\n",
      " 1   B       4 non-null      string \n",
      " 2   C       5 non-null      int8   \n",
      " 3   D       5 non-null      int8   \n",
      "dtypes: float16(1), int8(2), string(1)\n",
      "memory usage: 188.0 bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()\n",
    "df3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff7a7a0",
   "metadata": {},
   "source": [
    "Attempting to perform a left join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cef430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = df.merge(df3, how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c229d5eb",
   "metadata": {},
   "source": [
    "__Altering the values of a ``Sparse`` column:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "86277f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sparse = {\n",
    "    'A': [3, 6, np.nan, np.nan, np.nan],\n",
    "    'B': ['x', np.nan, 'z', 'a', 'e'],\n",
    "    'C': [1, 3, 4, 5, 6],\n",
    "    'D': [1,2,3,4,5]\n",
    "}\n",
    "\n",
    "df_sparse = pd.DataFrame(data_sparse)\n",
    "df_sparse2 = pd.DataFrame(data_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "196a677d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   A       2 non-null      float64\n",
      " 1   B       4 non-null      object \n",
      " 2   C       5 non-null      int64  \n",
      " 3   D       5 non-null      int64  \n",
      "dtypes: float64(1), int64(2), object(1)\n",
      "memory usage: 288.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df_sparse.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "01880077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total execution time was 0.002991914749145508 seconds\n",
      "Total execution time was 0.001995086669921875 seconds\n"
     ]
    }
   ],
   "source": [
    "df_sparse = optimize(df_sparse)\n",
    "df_sparse2 = optimize(df_sparse2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f67d2800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype               \n",
      "---  ------  --------------  -----               \n",
      " 0   A       2 non-null      Sparse[float64, nan]\n",
      " 1   B       4 non-null      string              \n",
      " 2   C       5 non-null      int8                \n",
      " 3   D       5 non-null      int8                \n",
      "dtypes: Sparse[float64, nan](1), int8(2), string(1)\n",
      "memory usage: 202.0 bytes\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype               \n",
      "---  ------  --------------  -----               \n",
      " 0   A       2 non-null      Sparse[float64, nan]\n",
      " 1   B       4 non-null      string              \n",
      " 2   C       5 non-null      int8                \n",
      " 3   D       5 non-null      int8                \n",
      "dtypes: Sparse[float64, nan](1), int8(2), string(1)\n",
      "memory usage: 202.0 bytes\n"
     ]
    }
   ],
   "source": [
    "df_sparse.info()\n",
    "df_sparse2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3238b7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sparse['A'] = list(i**2 for i in df_sparse['A']) # Using a generator expression\n",
    "df_sparse2['A'] = df_sparse2['A']**2 # Using a vectorized operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "90e69065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     9.0\n",
       "1    36.0\n",
       "2     NaN\n",
       "3     NaN\n",
       "4     NaN\n",
       "Name: A, dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sparse['A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5efd898e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     9.0\n",
       "1    36.0\n",
       "2     NaN\n",
       "3     NaN\n",
       "4     NaN\n",
       "Name: A, dtype: Sparse[float64, nan]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sparse2['A']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f38686",
   "metadata": {},
   "source": [
    "Notice how when we used a generator expression the dtype of the column is converted back to ``float64``. This is because when we create a list using a generator (or any other iterable), Python has no way of knowing that the original data was stored as a sparse array. So, when we assign that list back to your DataFrame, pandas will use the most general type that can accommodate all the data.\n",
    "\n",
    "Eventhough the main advantage of sparse arrays is that they allow you to perform computations while using less memory than dense arrays, we should be careful when performing operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63761dc",
   "metadata": {},
   "source": [
    "### Based on the considerations we have considered. Here is the final version of ``optimize()``. It doesn't use ``category``, instead it uses ``pyarrow`` backed ``StringArray`` for string columns. It skips if a column is all NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "52f19898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections.abc import Iterable\n",
    "import pyarrow\n",
    "\n",
    "SPARSITY_THRESHOLD = 0.5\n",
    "numeric_types = {'float64': {np.finfo(np.float16).max: np.float16,\n",
    "                             np.finfo(np.float32).max: np.float32},\n",
    "                 'int64': {np.iinfo(np.int8).max: np.int8,\n",
    "                           np.iinfo(np.int16).max: np.int16,\n",
    "                           np.iinfo(np.int32).max: np.int32}}\n",
    "\n",
    "def downcast_numeric(series):\n",
    "    max_val = series.max()\n",
    "    if pd.notna(max_val):\n",
    "        for max_type_val, type_val in numeric_types[series.dtype.name].items():\n",
    "            if max_val <= max_type_val:\n",
    "                return series.astype(type_val)\n",
    "    return series\n",
    "\n",
    "def convert_object(series):\n",
    "    # Try to convert the series to numeric\n",
    "    converted_series = pd.to_numeric(series, errors='coerce')\n",
    "\n",
    "    # If the series does not contain more np.nan after conversion, the conversion was successful\n",
    "    if series.isna().sum() >= converted_series.isna().sum():\n",
    "        series = downcast_numeric(converted_series)\n",
    "    else:\n",
    "        try:\n",
    "            series = pd.to_datetime(series)\n",
    "        except Exception:\n",
    "            series = series.astype(pd.StringDtype(storage='pyarrow'))\n",
    "\n",
    "    return series\n",
    "\n",
    "\n",
    "def optimize(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Minimizes memory usage by using smaller dtypes\n",
    "    :param df: dataframe input\n",
    "    :return: optimized df\n",
    "    \"\"\"\n",
    "\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_series = df[col]\n",
    "        col_dtype = col_series.dtype.name\n",
    "        \n",
    "        if col_series.isna().all():\n",
    "            continue\n",
    "        \n",
    "        if col_series.isna().mean() > SPARSITY_THRESHOLD:\n",
    "            df[col] = pd.arrays.SparseArray(col_series, dtype=col_dtype)\n",
    "        elif col_dtype == 'object' and not any(isinstance(val, Iterable) and not isinstance(val, str) for val in col_series.dropna()):\n",
    "            df[col] = convert_object(col_series)\n",
    "        elif col_dtype in numeric_types:\n",
    "            df[col] = downcast_numeric(col_series)\n",
    "        elif pd.api.types.is_sparse(col_series.dtype) and col_series.notna().mean() > 0.5:\n",
    "            df[col] = col_series.to_dense()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8134342b",
   "metadata": {},
   "source": [
    "#### b. Loading the data in chunks\n",
    "    \n",
    "  Loading the data in chunks could be useful when dealing with large datasets. By loading data in smaller portions, or           \"chunks\", memory usage is kept to a minimum, preventing potential slowdowns or crashes that could occur if the system runs     out of memory. This could be especially beneficial in environments where memory resources are limited. Also it gives the       opportunity to process the chunks independently. Hence, more flexible data processing is possible.\n",
    "  \n",
    "  However processing the data in chunks might end up in increased total processing time due to repeated disk operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f56544",
   "metadata": {},
   "source": [
    "##### ``fetchall()`` vs ``read_sql()``\n",
    "   The both functions load the data into memory at once. Pandas' ``read_sql()`` loads the data as a DataFrame, ``fetchall()``      of ``pyodbc`` or similar libraries load the data as a list of tuples. ``fetchall()`` could potentially use more memory when    the data contains columns with mixed types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7c730b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get current memory usage\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024**2  # return memory usage in MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bbf509",
   "metadata": {},
   "source": [
    "The ``get_memory_usage`` function returns the Resident Set Size (RSS) of the current process. RSS is the portion of the process's memory that is held in RAM.\n",
    "\n",
    "When a process is started, the operating system allocates a certain amount of physical memory (RAM) for it. This memory space is divided into several segments, each with a specific purpose:\n",
    "\n",
    "The RSS value refers to the portion of this memory which is in RAM, i.e., it includes the size of the stack, the heap, and the data segment of the process. It excludes memory that is swapped out to disk or memory-mapped files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "905af372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before loading data: 1466.52 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harun\\anaconda3\\lib\\site-packages\\pandas\\io\\sql.py:762: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after loading chunk 1: 1525.12 MB\n",
      "Chunk 1 size: 10.70 MB\n",
      "Memory usage after loading chunk 2: 1539.97 MB\n",
      "Chunk 2 size: 9.69 MB\n",
      "Memory usage after loading chunk 3: 1534.78 MB\n",
      "Chunk 3 size: 4.09 MB\n",
      "Memory usage after combining all chunks: 1537.83 MB\n",
      "Final DataFrame size: 24.48 MB\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import psutil\n",
    "\n",
    "# connect to the SQL Server database\n",
    "conn_str = (\n",
    "    r'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "    r'SERVER=localhost;'\n",
    "    r'DATABASE=AdventureWorks2019;'\n",
    "    r'Trusted_Connection=yes;'\n",
    ")\n",
    "cnxn = pyodbc.connect(conn_str)\n",
    "\n",
    "# define SQL query\n",
    "query1 = \"SELECT * FROM Sales.SalesOrderDetail\"\n",
    "\n",
    "# specify chunk size\n",
    "chunk_size = 50000\n",
    "\n",
    "# initialize an empty list to store chunks\n",
    "chunks = []\n",
    "\n",
    "print(f\"Memory usage before loading data: {get_memory_usage():.2f} MB\")\n",
    "\n",
    "# read and process data in chunks\n",
    "chunk_number = 0\n",
    "for chunk in pd.read_sql(query1, cnxn, chunksize=chunk_size):\n",
    "    # print memory usage for each chunk\n",
    "    chunk_number += 1\n",
    "    print(f\"Memory usage after loading chunk {chunk_number}: {get_memory_usage():.2f} MB\")\n",
    "    print(f\"Chunk {chunk_number} size: {chunk.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "    # process each chunk as a separate dataframe if needed\n",
    "    # here we simply add it to the list\n",
    "    chunks.append(chunk)\n",
    "\n",
    "# Combine all chunks into one DataFrame\n",
    "df1 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "print(f\"Memory usage after combining all chunks: {get_memory_usage():.2f} MB\")\n",
    "print(f\"Final DataFrame size: {df1.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "bb578292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after fetchall(): 1565.75 MB\n",
      "df1 memory 20462.578125\n",
      "After creating DataFrame from fetchall() results: 1568.20 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harun\\anaconda3\\lib\\site-packages\\pandas\\io\\sql.py:762: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df2 memory 20953680\n",
      "Memory usage after read_sql(): 1581.30 MB\n"
     ]
    }
   ],
   "source": [
    "# using fetchall()\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(\"SELECT * FROM Person.Person\")\n",
    "results = cursor.fetchall()\n",
    "print(f\"Memory usage after fetchall(): {get_memory_usage():.2f} MB\")\n",
    "\n",
    "# create dataframe from the results\n",
    "df1 = pd.DataFrame.from_records(results, columns=[desc[0] for desc in cursor.description])\n",
    "print(\"df1 memory\", df1.memory_usage(deep = True).sum()/1024)\n",
    "print(f\"After creating DataFrame from fetchall() results: {get_memory_usage():.2f} MB\")\n",
    "\n",
    "# using pandas.read_sql()\n",
    "df2 = pd.read_sql(\"SELECT * FROM Person.Person\", connection)\n",
    "print(\"df2 memory\", df2.memory_usage(deep = True).sum())\n",
    "print(f\"Memory usage after read_sql(): {get_memory_usage():.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3a70f2",
   "metadata": {},
   "source": [
    "## 2. Using NumPy arrays vs Pandas DataFrames\n",
    "\n",
    "A NumPy array is more memory-efficient than a pandas DataFrame. This is because a DataFrame has additional overhead due to its index and column label structures, as well as its ability to hold heterogeneous data types. A DataFrame essentially contains an underlying NumPy array, but also includes other data structures to support its extended functionality. Thus, if you have a large dataset composed of uniform data types and do not require the advanced functionalities provided by pandas, using a NumPy array could reduce your memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aa83ee",
   "metadata": {},
   "source": [
    "In the following script, the idea is to store the columns of the pandas dataframe as numpy arrays which reduces the memory usage drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "98e85dcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent creating numpy_arrays: 0.03900456428527832 seconds\n",
      "Memory usage of numpy array Unnamed: 0: 104 bytes\n",
      "Memory usage of numpy array column_0_cat: 104 bytes\n",
      "Memory usage of numpy array column_1_int: 104 bytes\n",
      "Memory usage of numpy array column_2_float: 104 bytes\n",
      "Memory usage of numpy array column_3_int: 104 bytes\n",
      "Memory usage of numpy array column_4_float: 104 bytes\n",
      "Memory usage of numpy array column_5_date: 40000104 bytes\n",
      "Memory usage of numpy array column_6_int: 104 bytes\n",
      "Memory usage of numpy array column_7_cat: 104 bytes\n",
      "Memory usage of numpy array column_8_cat: 104 bytes\n",
      "Memory usage of numpy array column_9_date: 104 bytes\n",
      "\n",
      "Time spent creating df2: 0.007978677749633789 seconds\n",
      "\n",
      "Time spent creating df3: 0.38344287872314453 seconds\n",
      "\n",
      "Memory usage of original df: 1426.738751411438 mbytes\n",
      "\n",
      "Memory usage of new df2: 38.14826965332031 mbytes\n",
      "\n",
      "Memory usage of new df3: 1736.6516065597534 mbytes\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import time\n",
    "#df = optimize(df)\n",
    "\n",
    "# Create a dictionary to store numpy arrays\n",
    "numpy_arrays = {}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Iterate over columns and create numpy arrays\n",
    "for column in df.columns:\n",
    "    # Note the additional list() wrapping\n",
    "    numpy_arrays[column] = list([df[column].to_numpy()])\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time spent creating numpy_arrays: {end_time - start_time} seconds \\n\")\n",
    "\n",
    "# Print memory usage of each numpy array\n",
    "for name, array in numpy_arrays.items():\n",
    "    print(f\"Memory usage of numpy array {name}: {getsizeof(array[0])} bytes\")\n",
    "    \n",
    "start_time = time.time()\n",
    "\n",
    "# Create a new DataFrame with numpy arrays at every column as single cells\n",
    "df2 = pd.DataFrame(numpy_arrays)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nTime spent creating df2: {end_time - start_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Creating a new DataFrame from the numpy arrays, with the same dtypes of columns of df\n",
    "df3 = pd.DataFrame({col: pd.Series(arr[0]) for col, arr in numpy_arrays.items()})\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nTime spent creating df3: {end_time - start_time} seconds\")\n",
    "\n",
    "# Compare memory usage\n",
    "print(f\"\\nMemory usage of original df: {df.memory_usage(deep=True).sum() / 1024**2} mbytes\")\n",
    "print(f\"\\nMemory usage of new df2: {df2.memory_usage(deep=True).sum() / 1024**2} mbytes\")\n",
    "print(f\"\\nMemory usage of new df3: {df3.memory_usage(deep=True).sum() / 1024**2} mbytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419f70b2",
   "metadata": {},
   "source": [
    "Although a dataframe as numpy arrays consume little memory, since there is no straightforward way of perform pandas operations for data manipulation using numpy arrays, we need to create a dataframe from the numpy arrays again at some point. \n",
    "\n",
    "Converting data between pandas and NumPy involves overhead, both in terms of computational resources and in terms of code complexity. \n",
    "\n",
    "We can see that it takes a considerable amount of time when creating df3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a797f030",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 1 (5000000 rows, 1 column of integers):\n",
      "\tPandas memory_usage: 39062.625 bytes\n",
      "\n",
      "DataFrame 2 (5000000 rows, 1 column of single-element numpy array):\n",
      "\tPandas memory_usage: 19531.375 bytes\n",
      "\n",
      "DataFrame 3 (1 row, 1 column of numpy array with 5000000 elements):\n",
      "\tPandas memory_usage: 19531.484375 bytes\n",
      "\n",
      "DataFrame 3 (1 row, 1 column of numpy array with 5000000 elements):\n",
      "\tPandas memory_usage: 9765.859375 bytes\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame 1: 500000 rows, 1 column of integer type\n",
    "df1 = pd.DataFrame({'col': range(5000000)})\n",
    "df1_memory_usage = df1.memory_usage(deep=True).sum()\n",
    "df1_sys_memory_usage = getsizeof(df1)\n",
    "print(f\"DataFrame 1 (5000000 rows, 1 column of integers):\")\n",
    "print(f\"\\tPandas memory_usage: {df1_memory_usage/1024} bytes\")\n",
    "\n",
    "# Create DataFrame 2: 5000000 rows, 1 column of single-element NumPy array\n",
    "arr = np.array(range(5000000))\n",
    "df2 = pd.DataFrame({'col': arr})\n",
    "df2_memory_usage = df2.memory_usage(deep=True).sum()\n",
    "df2_sys_memory_usage = getsizeof(df2)\n",
    "print(f\"\\nDataFrame 2 (5000000 rows, 1 column of single-element numpy array):\")\n",
    "print(f\"\\tPandas memory_usage: {df2_memory_usage/1024} bytes\")\n",
    "\n",
    "# Create DataFrame 3: 1 row, 1 column with a NumPy array with 5000000 elements\n",
    "df3 = pd.DataFrame({'col': [np.array(range(5000000))]})\n",
    "df3_memory_usage = df3.memory_usage(deep=True).sum()\n",
    "df3_sys_memory_usage = getsizeof(df3)\n",
    "print(f\"\\nDataFrame 3 (1 row, 1 column of numpy array with 5000000 elements):\")\n",
    "print(f\"\\tPandas memory_usage: {df3_memory_usage/1024} bytes\")\n",
    "\n",
    "# Converting the dtype of the np array of df3 to int16\n",
    "df3['col'] = df3['col'].apply(lambda x: x.astype('int16'))\n",
    "df3_memory_usage = df3.memory_usage(deep=True).sum()\n",
    "df3_sys_memory_usage = getsizeof(df3)\n",
    "print(f\"\\nDataFrame 3 (1 row, 1 column of numpy array with 5000000 elements):\")\n",
    "print(f\"\\tPandas memory_usage: {df3_memory_usage/1024} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e58b5b2",
   "metadata": {},
   "source": [
    "## 3. Dask DataFrames vs Pandas DataFrames\n",
    "\n",
    "Dask DataFrames are a large parallel DataFrame composed of smaller Pandas DataFrames. The large DataFrame is partitioned into several smaller chunks, where each chunk is a valid DataFrame itself. This allowes for distributed computation behind the scenes. Dask DataFrames support a large subset of the Pandas API, including groupbys, join operations, and sophisticated time series manipulations. Importantly, Dask operations are lazily evaluated, meaning computations are not executed until the result is explicitly requested. \n",
    "\n",
    "__Using Dask and Pandas Interchangeably:__ This can be a powerful strategy for dealing with memory limitations. The reason is Dask allows lazy evalutaion, which means computations are not performed until necessary, hence saving memory.  the ``compute()`` method is where all the computations take place. This can potentially save a lot of memory because data isn't loaded until necessary.\n",
    "\n",
    "Whenever an operation that is not supported by Dask is required, the Dask DataFrame can be converted to a Pandas DataFrame. After performing the operation, the result can be converted back into a Dask DataFrame. This method leverages the strengths of both libraries, while avoiding memory overflow issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb47045",
   "metadata": {},
   "source": [
    "__Some fundamental Dask DataFrame arguments:__ The ``npartitions`` parameter specifies how many partitions you want to divide your Dask DataFrame into. For example, if you set ``npartitions=5``, your Dask DataFrame will consist of 5 smaller Pandas DataFrames. Bear in mind that having too few partitions could limit parallelism, having too many partitions can lead to slow task scheduling and increased memory usage.\n",
    "\n",
    "In general, a good rule of thumb is to create partitions that are at least a few tens of megabytes in size, up to a maximum size that fits comfortably in memory. You might start with npartitions equal to twice the number of your machine's CPU cores and then adjust as necessary based on the memory usage and computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39455f02",
   "metadata": {},
   "source": [
    "Here is a script, comparing the time spent in operations where we retrieve data and perform merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "79554365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harun\\anaconda3\\lib\\site-packages\\pandas\\io\\sql.py:762: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to load data into pandas dataframes and convert to dask df: 2.1651906967163086 seconds\n",
      "Time taken to merge dask dataframes: 0.029373884201049805 seconds\n",
      "Time taken to convert merged dask dataframe back to pandas: 0.3315107822418213 seconds\n",
      "\n",
      "Total time taken in dask: 2.5260753631591797 seconds\n",
      "\n",
      "Time taken to load and merge dataframes using only pandas: 2.13199520111084 seconds\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import time\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "dask_total_time = 0\n",
    "\n",
    "# SQL Server connection string\n",
    "conn_str = (\n",
    "    r'mssql+pyodbc:///?odbc_connect=' +\n",
    "    r'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "    r'SERVER=localhost;'\n",
    "    r'DATABASE=AdventureWorks2019;'\n",
    "    r'Trusted_Connection=yes;'\n",
    ")\n",
    "\n",
    "engine = create_engine(conn_str)\n",
    "\n",
    "# Load the data into pandas dataframes\n",
    "start = time.time()\n",
    "query1 = \"SELECT * FROM Sales.SalesOrderDetail\"\n",
    "#df1 = pd.read_sql(query1, engine)\n",
    "ddf1 = dd.from_pandas(pd.read_sql(query1, engine), npartitions=5)\n",
    "\n",
    "query2 = \"SELECT * FROM Sales.SalesOrderHeader\"\n",
    "#df2 = pd.read_sql(query2, engine)\n",
    "ddf2 = dd.from_pandas(pd.read_sql(query2, cnxn), npartitions=5)\n",
    "end = time.time()\n",
    "print(f\"Time taken to load data into pandas dataframes and convert to dask df: {end-start} seconds\")\n",
    "dask_total_time += (end-start)\n",
    "\n",
    "# Merge operation in Dask\n",
    "start = time.time()\n",
    "merged_ddf = dd.merge(ddf1, ddf2, on='SalesOrderID', how='left')\n",
    "end = time.time()\n",
    "print(f\"Time taken to merge dask dataframes: {end-start} seconds\")\n",
    "dask_total_time += (end-start)\n",
    "\n",
    "# Convert merged dask dataframe back to pandas\n",
    "start = time.time()\n",
    "merged_df = merged_ddf.compute()\n",
    "end = time.time()\n",
    "print(f\"Time taken to convert merged dask dataframe back to pandas: {end-start} seconds\")\n",
    "dask_total_time += (end-start)\n",
    "\n",
    "print()\n",
    "print(f\"Total time taken in dask: {dask_total_time} seconds\")\n",
    "print()\n",
    "\n",
    "# Convert merged dataframe to dask dataframe\n",
    "#start = time.time()\n",
    "#ddf_merged = dd.from_pandas(merged_df, npartitions=2)\n",
    "#end = time.time()\n",
    "#print(f\"Time taken to convert merged pandas dataframe to dask: {end-start} seconds\")\n",
    "\n",
    "# Doing the same operation with pandas only\n",
    "start = time.time()\n",
    "query1 = \"SELECT * FROM Sales.SalesOrderDetail\"\n",
    "df1_pandas = pd.read_sql(query1, engine)\n",
    "\n",
    "query2 = \"SELECT * FROM Sales.SalesOrderHeader\"\n",
    "df2_pandas = pd.read_sql(query2, engine)\n",
    "\n",
    "merged_df_pandas = pd.merge(df1_pandas, df2_pandas, on='SalesOrderID', how='left')\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken to load and merge dataframes using only pandas: {end-start} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3262ac19",
   "metadata": {},
   "source": [
    "Using dask and pandas interchangeably is a little bit longer, but we load the data into memory as a whole only when it is necessary with ``compute()``. Reducing the probability of using too much memory during concurrent operations (say you have lots of requests coming in and memory usage of different operations add up and exceed the limit).\n",
    "\n",
    "__Note that it could be faster if we load the data directly into a dask dataframe using ``read_sql_table``.__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
