{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eefa9e2a",
   "metadata": {},
   "source": [
    "# This notebook is a PoC attempt on memory usage optimization in Pandas\n",
    "\n",
    "### Contents:\n",
    "\n",
    "#####      1. Optimization directly on pandas dataframes\n",
    "#####      2. Using NumPy arrays vs Pandas DataFrames\n",
    "#####      3. Dask DataFrames vs Pandas DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c5c780",
   "metadata": {},
   "source": [
    "## 1. Optimization directly on pandas dataframes\n",
    "\n",
    "   #### a. Alter the column dtypes:\n",
    "    \n",
    "  When we create a Pandas DataFrame, Pandas will assign the highest memory datatype to columns by default. For example, when     it detects a column of integers it will assign ```int64``` to the dtype of the column, regardless of the size of the integer   values. This consumes a lot of unnecessary memory. We can use the ```astype()``` method to downgrade the datatypes. Here is     an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f72f233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "51fea205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Method to generate dummy dataframe that we will use throughout the notebook.\n",
    "\n",
    "def generate_fake_dataframe(size, cols, col_names = None, intervals = None, seed = None):\n",
    "    \n",
    "    categories_dict = {'animals': ['cow', 'rabbit', 'duck', 'shrimp', 'pig', 'goat', 'crab', 'deer', 'bee', 'sheep', 'fish', 'turkey', 'dove', 'chicken', 'horse'],\n",
    "                       'names'  : ['James', 'Mary', 'Robert', 'Patricia', 'John', 'Jennifer', 'Michael', 'Linda', 'William', 'Elizabeth', 'Ahmed', 'Barbara', 'Richard', 'Susan', 'Salomon', 'Juan Luis'],\n",
    "                       'cities' : ['Stockholm', 'Denver', 'Moscow', 'Marseille', 'Palermo', 'Tokyo', 'Lisbon', 'Oslo', 'Nairobi', 'Río de Janeiro', 'Berlin', 'Bogotá', 'Manila', 'Madrid', 'Milwaukee'],\n",
    "                       'colors' : ['red', 'orange', 'yellow', 'green', 'blue', 'indigo', 'purple', 'pink', 'silver', 'gold', 'beige', 'brown', 'grey', 'black', 'white']\n",
    "                      }\n",
    "    default_intervals = {\"i\" : (0,10), \"f\" : (0,100), \"c\" : (\"names\", 5), \"d\" : (\"2020-01-01\",\"2020-12-31\")}\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    first_c = default_intervals[\"c\"][0]\n",
    "    categories_names = cycle([first_c] + [c for c in categories_dict.keys() if c != first_c])\n",
    "    default_intervals[\"c\"] = (categories_names, default_intervals[\"c\"][1])\n",
    "    \n",
    "    if isinstance(col_names,list):\n",
    "        assert len(col_names) == len(cols), f\"The fake DataFrame should have {len(cols)} columns but col_names is a list with {len(col_names)} elements\"\n",
    "    elif col_names is None:\n",
    "        suffix = {\"c\" : \"cat\", \"i\" : \"int\", \"f\" : \"float\", \"d\" : \"date\"}\n",
    "        col_names = [f\"column_{str(i)}_{suffix.get(col)}\" for i, col in enumerate(cols)]\n",
    "\n",
    "    if isinstance(intervals,list):\n",
    "        assert len(intervals) == len(cols), f\"The fake DataFrame should have {len(cols)} columns but intervals is a list with {len(intervals)} elements\"\n",
    "    else:\n",
    "        if isinstance(intervals,dict):\n",
    "            assert len(set(intervals.keys()) - set(default_intervals.keys())) == 0, f\"The intervals parameter has invalid keys\"\n",
    "            default_intervals.update(intervals)\n",
    "        intervals = [default_intervals[col] for col in cols]\n",
    "    df = pd.DataFrame()\n",
    "    for col, col_name, interval in zip(cols, col_names, intervals):\n",
    "        if interval is None:\n",
    "            interval = default_intervals[col]\n",
    "        assert (len(interval) == 2 and isinstance(interval, tuple)) or isinstance(interval, list), f\"This interval {interval} is neither a tuple of two elements nor a list of strings.\"\n",
    "        if col in (\"i\",\"f\",\"d\"):\n",
    "            start, end = interval\n",
    "        if col == \"i\":\n",
    "            df[col_name] = rng.integers(start, end, size)\n",
    "        elif col == \"f\":\n",
    "            df[col_name] = rng.uniform(start, end, size)\n",
    "        elif col == \"c\":\n",
    "            if isinstance(interval, list):\n",
    "                categories = np.array(interval)\n",
    "            else:\n",
    "                cat_family, length = interval\n",
    "                if isinstance(cat_family, cycle):\n",
    "                    cat_family = next(cat_family)\n",
    "                assert cat_family in categories_dict.keys(), f\"There are no samples for category '{cat_family}'. Consider passing a list of samples or use one of the available categories: {categories_dict.keys()}\"\n",
    "                categories = rng.choice(categories_dict[cat_family], length, replace = False, shuffle = True)\n",
    "            df[col_name] = rng.choice(categories, size, shuffle = True)\n",
    "        elif col == \"d\":\n",
    "            df[col_name] = rng.choice(pd.date_range(start, end), size)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fe4083",
   "metadata": {},
   "source": [
    "__This method is fetched from the article [Generating fake data with pandas, very quickly](https://towardsdatascience.com/generating-fake-data-with-pandas-very-quickly-b99467d4c618)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b20e7039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a dummy data frame with 5000000 rows and 10 columns.\n",
    "# 'c' = category, 'i' = integer, 'f' = float, 'd' = datetime.\n",
    "\n",
    "dummy_df = generate_fake_dataframe(size = 5000000, cols =  \"cififdiccd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ea0ca9ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000000 entries, 0 to 4999999\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Dtype         \n",
      "---  ------          -----         \n",
      " 0   column_0_cat    object        \n",
      " 1   column_1_int    int64         \n",
      " 2   column_2_float  float64       \n",
      " 3   column_3_int    int64         \n",
      " 4   column_4_float  float64       \n",
      " 5   column_5_date   datetime64[ns]\n",
      " 6   column_6_int    int64         \n",
      " 7   column_7_cat    object        \n",
      " 8   column_8_cat    object        \n",
      " 9   column_9_date   datetime64[ns]\n",
      "dtypes: datetime64[ns](2), float64(2), int64(3), object(3)\n",
      "memory usage: 1.2 GB\n"
     ]
    }
   ],
   "source": [
    "# Info about the data frame\n",
    "dummy_df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c54ce5f",
   "metadata": {},
   "source": [
    "As I have mentioned before, every datatype is the largest type as possible. The memory usage is over 1 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "61e01772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the data frame as csv\n",
    "df_as_csv = dummy_df.to_csv('out.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f9cac6",
   "metadata": {},
   "source": [
    "This might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c9c649e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of the csv\n",
    "data_dir = '.\\out.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4c303522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv\n",
    "df = pd.read_csv(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "71f2f5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000000 entries, 0 to 4999999\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Dtype  \n",
      "---  ------          -----  \n",
      " 0   Unnamed: 0      int64  \n",
      " 1   column_0_cat    object \n",
      " 2   column_1_int    int64  \n",
      " 3   column_2_float  float64\n",
      " 4   column_3_int    int64  \n",
      " 5   column_4_float  float64\n",
      " 6   column_5_date   object \n",
      " 7   column_6_int    int64  \n",
      " 8   column_7_cat    object \n",
      " 9   column_8_cat    object \n",
      " 10  column_9_date   object \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 1.8 GB\n"
     ]
    }
   ],
   "source": [
    "# Df info\n",
    "df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5452f9b3",
   "metadata": {},
   "source": [
    "Notice that the datetime columns are now type of object. Now it consumes even more memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9e10aab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index                   128\n",
       "Unnamed: 0         40000000\n",
       "column_0_cat      318994442\n",
       "column_1_int       40000000\n",
       "column_2_float     40000000\n",
       "column_3_int       40000000\n",
       "column_4_float     40000000\n",
       "column_5_date     335000000\n",
       "column_6_int       40000000\n",
       "column_7_cat      308996545\n",
       "column_8_cat      348020080\n",
       "column_9_date     335000000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Memory usage of each column\n",
    "df.memory_usage(index = True, deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0eb393d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall memory consumption: 1798.6404371261597 MB\n"
     ]
    }
   ],
   "source": [
    "# Overall memory consumption\n",
    "memory_consumption = df.memory_usage(index = True, deep = True).sum() / 1024**2\n",
    "print(f\"Overall memory consumption: {memory_consumption} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cce8d11",
   "metadata": {},
   "source": [
    "#### In order to save some memory, we can inspect the columns and alter the datatypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff5b77b",
   "metadata": {},
   "source": [
    " Take the \"column_1_int\" column. It is of type ```int64```. Let's check out the max and min values and see if they \n",
    " span the scope of the datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b5addb56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of column_1_int column is int64\n",
      "Maximum value in column_1_int column is 9\n",
      "Minimum value in column_1_int column is 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Data type of column_1_int column is\", df.column_1_int.dtype)\n",
    "print(\"Maximum value in column_1_int column is\", df.column_1_int.max())\n",
    "print(\"Minimum value in column_1_int column is\", df.column_1_int.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e7513f",
   "metadata": {},
   "source": [
    "As we can see the column has only integers between 0-10 but the datatype is still ```int64```. Now let's cast the dtype to ````int8```` and see the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "51e0825e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing the datatype: 40000128\n",
      "Memory usage after changing the datatype: 5000128\n"
     ]
    }
   ],
   "source": [
    "print(\"Memory usage before changing the datatype:\", df.column_1_int.memory_usage(deep = True))\n",
    "\n",
    "df[\"column_1_int\"] = df.column_1_int.astype(np.int8)\n",
    "\n",
    "print(\"Memory usage after changing the datatype:\", df.column_1_int.memory_usage(deep = True))\n",
    "\n",
    "# It has reduced the memory usage almost by 88%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19020f5d",
   "metadata": {},
   "source": [
    "__It has reduced the memory usage almost by 88%__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9913b594",
   "metadata": {},
   "source": [
    " Take the \"column_2_float\" column. It is of type ``float64``. Let's check out the max and min values and see if they \n",
    " span the scope of the datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1c08d0ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of column_2_float column is float64\n",
      "Maximum value in column_2_float column is 99.99999248133842\n",
      "Minimum value in column_2_float column is 9.635407993702216e-06\n"
     ]
    }
   ],
   "source": [
    "print(\"Data type of column_2_float column is\", df.column_2_float.dtype)\n",
    "print(\"Maximum value in column_2_float column is\", df.column_2_float.max())\n",
    "print(\"Minimum value in column_2_float column is\", df.column_2_float.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4591afc",
   "metadata": {},
   "source": [
    "As we can see the column has only floating point numbers between 0-10 but the datatype is still ```float64```. Now let's cast the dtype to ````float16```` and see the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4d3e6067",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing the datatype: 40000128\n",
      "Memory usage after changing the datatype: 10000128\n"
     ]
    }
   ],
   "source": [
    "print(\"Memory usage before changing the datatype:\", df.column_2_float.memory_usage(deep = True))\n",
    "\n",
    "df[\"column_2_float\"] = df.column_2_float.astype(np.float16)\n",
    "\n",
    "print(\"Memory usage after changing the datatype:\", df.column_2_float.memory_usage(deep = True))\n",
    "\n",
    "# It has reduced the memory usage almost by 75%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6a3391",
   "metadata": {},
   "source": [
    "#### Pandas assign \"object\" for columns with categorical data. \n",
    "#### If we have a column of type \"object\" and has few unique values, we can alter the dtype to \"categorical\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "35ff6bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of column_5_date column is object\n",
      "Number of Unique values in column_5_date column is 366\n",
      "The number of rows 5000000\n"
     ]
    }
   ],
   "source": [
    "# Let's see the number of unique values in 'column_5_date' column\n",
    "\n",
    "print(\"Data type of column_5_date column is\", df.column_5_date.dtype)\n",
    "print(\"Number of Unique values in column_5_date column is\", df.column_5_date.nunique())\n",
    "print(\"The number of rows\", df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d255c7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before changing the datatype: 40000128\n",
      "Memory usage after changing the datatype: 10011352\n"
     ]
    }
   ],
   "source": [
    "# We have 5000000 values in the 'column_5_date' column but only 366 of them are unique.\n",
    "# It means we can represent this column as 'categorical'\n",
    "\n",
    "print(\"Memory usage before changing the datatype:\", df.column_5_date.memory_usage())\n",
    "\n",
    "df[\"column_5_date\"] = df.column_5_date.astype(\"category\")\n",
    "\n",
    "print(\"Memory usage after changing the datatype:\", df.column_5_date.memory_usage())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8f2a42",
   "metadata": {},
   "source": [
    "#### There are some considerations to keep in mind when representing datetime values as categorical\n",
    "**Loss of precision:** Categorical data type represents data using a numerical code associated with each unique category. By converting datetime values to categorical, you may lose the precision of the original timestamps. If you require high precision for your datetime calculations, it's better to use the 'datetime' data type.\n",
    "\n",
    "**Limited functionality:** Categorical data type provides benefits in terms of memory optimization and faster operations on the column. However, some datetime-specific operations and functions may not be available or may behave differently when applied to categorical datetime values. If your analysis heavily relies on datetime functionality, it's advisable to keep the column as 'datetime' type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c7ce2e",
   "metadata": {},
   "source": [
    "Here is a util that does the downcasting for you. Also when the majority of the values are missing in a column. It keeps that column in a Sparse Array, which saves up some memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "be6cba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARSITY_THRESHOLD = 0.5\n",
    "\n",
    "def optimize(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    float_types = {np.finfo(np.float16).max: np.float16, np.finfo(np.float32).max: np.float32}\n",
    "    int_types = {np.iinfo(np.int8).max: np.int8, np.iinfo(np.int16).max: np.int16, np.iinfo(np.int32).max: np.int32}\n",
    "    \n",
    "    for dtype in ['float64', 'int64']:\n",
    "        selected_columns = df.select_dtypes(include=[dtype])\n",
    "        max_values = selected_columns.max()\n",
    "        types = float_types if dtype == 'float64' else int_types\n",
    "\n",
    "        for col in selected_columns.columns:\n",
    "            if df[col].isna().sum() / len(df[col]) > SPARSITY_THRESHOLD:  # If mostly NaN values\n",
    "                df[col] = pd.arrays.SparseArray(df[col])\n",
    "            else:\n",
    "                max_val = max_values[col]\n",
    "                if not pd.isna(max_val):  # skip columns with only NaNs\n",
    "                    for max_type_val, type_val in types.items():\n",
    "                        if max_val <= max_type_val:\n",
    "                            df[col] = df[col].astype(type_val)\n",
    "                            break\n",
    "\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        if df[col].isna().sum() / len(df[col]) > SPARSITY_THRESHOLD:  # If mostly NaN values\n",
    "            df[col] = pd.arrays.SparseArray(df[col])\n",
    "        else:\n",
    "            try:\n",
    "                df[col] = pd.to_datetime(df[col])\n",
    "            except ValueError:\n",
    "                num_unique_values = len(df[col].unique())\n",
    "                num_total_values = len(df[col])\n",
    "                if num_unique_values / num_total_values < SPARSITY_THRESHOLD:\n",
    "                    df[col] = df[col].astype('category')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5905d532",
   "metadata": {},
   "source": [
    "__Here is a demonstration of the ``optimize()`` method__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "6fcff661",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000000 entries, 0 to 4999999\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Dtype  \n",
      "---  ------          -----  \n",
      " 0   Unnamed: 0      int64  \n",
      " 1   column_0_cat    object \n",
      " 2   column_1_int    int64  \n",
      " 3   column_2_float  float64\n",
      " 4   column_3_int    int64  \n",
      " 5   column_4_float  float64\n",
      " 6   column_5_date   object \n",
      " 7   column_6_int    int64  \n",
      " 8   column_7_cat    object \n",
      " 9   column_8_cat    object \n",
      " 10  column_9_date   object \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 1.8 GB\n",
      "None\n",
      "***********************\n",
      "Memory usage of df before optimization: 1798.64 MB\n",
      "**************************************************\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000000 entries, 0 to 4999999\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Dtype         \n",
      "---  ------          -----         \n",
      " 0   Unnamed: 0      int32         \n",
      " 1   column_0_cat    category      \n",
      " 2   column_1_int    int8          \n",
      " 3   column_2_float  float16       \n",
      " 4   column_3_int    int8          \n",
      " 5   column_4_float  float16       \n",
      " 6   column_5_date   datetime64[ns]\n",
      " 7   column_6_int    int8          \n",
      " 8   column_7_cat    category      \n",
      " 9   column_8_cat    category      \n",
      " 10  column_9_date   datetime64[ns]\n",
      "dtypes: category(3), datetime64[ns](2), float16(2), int32(1), int8(3)\n",
      "memory usage: 143.1 MB\n",
      "None\n",
      "***********************\n",
      "Memory usage of df before optimization: 143.05 MB\n"
     ]
    }
   ],
   "source": [
    "# Trying out the optimize() method\n",
    "\n",
    "df_temp = pd.read_csv(data_dir)\n",
    "print(df_temp.info(memory_usage = \"deep\"))\n",
    "print(\"***********************\")\n",
    "print(\"Memory usage of df before optimization: {:.2f} MB\".format(df_temp.memory_usage(deep=True).sum() / 1024**2))\n",
    "\n",
    "print(\"**************************************************\")\n",
    "\n",
    "df_temp = optimize(df_temp)\n",
    "print(df_temp.info(memory_usage = \"deep\"))\n",
    "print(\"***********************\")\n",
    "print(\"Memory usage of df before optimization: {:.2f} MB\".format(df_temp.memory_usage(deep=True).sum() / 1024**2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ab3d0f",
   "metadata": {},
   "source": [
    "__Simulating a case:__ Merging the optimized dataframe with another dataframe that has an integer value larger than ``int8`` under a column with the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8af0f572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   column_0_cat    5000 non-null   object        \n",
      " 1   column_1_int    5000 non-null   int64         \n",
      " 2   column_2_float  5000 non-null   float64       \n",
      " 3   column_3_int    5000 non-null   int64         \n",
      " 4   column_4_float  5000 non-null   float64       \n",
      " 5   column_5_date   5000 non-null   datetime64[ns]\n",
      " 6   column_6_int    5000 non-null   int64         \n",
      " 7   column_7_cat    5000 non-null   object        \n",
      " 8   column_8_cat    5000 non-null   object        \n",
      " 9   column_9_date   5000 non-null   datetime64[ns]\n",
      "dtypes: datetime64[ns](2), float64(2), int64(3), object(3)\n",
      "memory usage: 390.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Generating a dummy data frame with 5000 rows and 10 columns. Merging two dataframes with 5 million rows will exceed the memory\n",
    "# constraints.\n",
    "\n",
    "dummy_df_2 = generate_fake_dataframe(size = 5000, cols =  \"cififdiccd\")\n",
    "df_optimized = dummy_df_2\n",
    "dummy_df_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "06110f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   column_0_cat    5000 non-null   category      \n",
      " 1   column_1_int    5000 non-null   int32         \n",
      " 2   column_2_float  5000 non-null   float16       \n",
      " 3   column_3_int    5000 non-null   int8          \n",
      " 4   column_4_float  5000 non-null   float16       \n",
      " 5   column_5_date   5000 non-null   datetime64[ns]\n",
      " 6   column_6_int    5000 non-null   int8          \n",
      " 7   column_7_cat    5000 non-null   category      \n",
      " 8   column_8_cat    5000 non-null   category      \n",
      " 9   column_9_date   5000 non-null   datetime64[ns]\n",
      "dtypes: category(3), datetime64[ns](2), float16(2), int32(1), int8(2)\n",
      "memory usage: 142.3 KB\n"
     ]
    }
   ],
   "source": [
    "df_optimized = optimize(df_optimized)\n",
    "df_optimized.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "8a9ba96b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_0_cat</th>\n",
       "      <th>column_1_int</th>\n",
       "      <th>column_2_float</th>\n",
       "      <th>column_3_int</th>\n",
       "      <th>column_4_float</th>\n",
       "      <th>column_5_date</th>\n",
       "      <th>column_6_int</th>\n",
       "      <th>column_7_cat</th>\n",
       "      <th>column_8_cat</th>\n",
       "      <th>column_9_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Patricia</td>\n",
       "      <td>4</td>\n",
       "      <td>24.7500</td>\n",
       "      <td>4</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>2020-08-05</td>\n",
       "      <td>1</td>\n",
       "      <td>horse</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>2020-04-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Michael</td>\n",
       "      <td>0</td>\n",
       "      <td>79.0625</td>\n",
       "      <td>9</td>\n",
       "      <td>95.125000</td>\n",
       "      <td>2020-04-04</td>\n",
       "      <td>5</td>\n",
       "      <td>pig</td>\n",
       "      <td>Nairobi</td>\n",
       "      <td>2020-08-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Michael</td>\n",
       "      <td>10000000</td>\n",
       "      <td>78.4375</td>\n",
       "      <td>6</td>\n",
       "      <td>1.405273</td>\n",
       "      <td>2020-11-13</td>\n",
       "      <td>8</td>\n",
       "      <td>pig</td>\n",
       "      <td>Lisbon</td>\n",
       "      <td>2020-02-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Susan</td>\n",
       "      <td>4</td>\n",
       "      <td>79.3125</td>\n",
       "      <td>1</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>2020-09-10</td>\n",
       "      <td>7</td>\n",
       "      <td>cow</td>\n",
       "      <td>Bogotá</td>\n",
       "      <td>2020-06-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Michael</td>\n",
       "      <td>4</td>\n",
       "      <td>85.7500</td>\n",
       "      <td>9</td>\n",
       "      <td>72.562500</td>\n",
       "      <td>2020-06-13</td>\n",
       "      <td>4</td>\n",
       "      <td>fish</td>\n",
       "      <td>Bogotá</td>\n",
       "      <td>2020-02-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  column_0_cat  column_1_int  column_2_float  column_3_int  column_4_float  \\\n",
       "0     Patricia             4         24.7500             4       57.000000   \n",
       "1      Michael             0         79.0625             9       95.125000   \n",
       "2      Michael      10000000         78.4375             6        1.405273   \n",
       "3        Susan             4         79.3125             1       85.000000   \n",
       "4      Michael             4         85.7500             9       72.562500   \n",
       "\n",
       "  column_5_date  column_6_int column_7_cat column_8_cat column_9_date  \n",
       "0    2020-08-05             1        horse    Marseille    2020-04-27  \n",
       "1    2020-04-04             5          pig      Nairobi    2020-08-19  \n",
       "2    2020-11-13             8          pig       Lisbon    2020-02-09  \n",
       "3    2020-09-10             7          cow       Bogotá    2020-06-22  \n",
       "4    2020-06-13             4         fish       Bogotá    2020-02-03  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inserting a large integer number\n",
    "\n",
    "df_to_merge = dummy_df_2\n",
    "df_to_merge.at[2, 'column_1_int'] = 10000000\n",
    "df_to_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "807636b8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Performing a left join\n",
    "merged_df = pd.merge(df_to_merge, df_optimized, on='column_1_int', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "012ae6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2502224 entries, 0 to 2502223\n",
      "Data columns (total 19 columns):\n",
      " #   Column            Dtype         \n",
      "---  ------            -----         \n",
      " 0   column_0_cat_x    category      \n",
      " 1   column_1_int      int32         \n",
      " 2   column_2_float_x  float16       \n",
      " 3   column_3_int_x    int8          \n",
      " 4   column_4_float_x  float16       \n",
      " 5   column_5_date_x   datetime64[ns]\n",
      " 6   column_6_int_x    int8          \n",
      " 7   column_7_cat_x    category      \n",
      " 8   column_8_cat_x    category      \n",
      " 9   column_9_date_x   datetime64[ns]\n",
      " 10  column_0_cat_y    category      \n",
      " 11  column_2_float_y  float16       \n",
      " 12  column_3_int_y    int8          \n",
      " 13  column_4_float_y  float16       \n",
      " 14  column_5_date_y   datetime64[ns]\n",
      " 15  column_6_int_y    int8          \n",
      " 16  column_7_cat_y    category      \n",
      " 17  column_8_cat_y    category      \n",
      " 18  column_9_date_y   datetime64[ns]\n",
      "dtypes: category(6), datetime64[ns](4), float16(4), int32(1), int8(4)\n",
      "memory usage: 148.0 MB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info(memory_usage = \"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1592a6",
   "metadata": {},
   "source": [
    "As we can see the datatypes are back to being unefficient. Optimizing again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "746194bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = optimize(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b63a52f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2502224 entries, 0 to 2502223\n",
      "Data columns (total 19 columns):\n",
      " #   Column            Dtype         \n",
      "---  ------            -----         \n",
      " 0   column_0_cat_x    category      \n",
      " 1   column_1_int      int32         \n",
      " 2   column_2_float_x  float16       \n",
      " 3   column_3_int_x    int8          \n",
      " 4   column_4_float_x  float16       \n",
      " 5   column_5_date_x   datetime64[ns]\n",
      " 6   column_6_int_x    int8          \n",
      " 7   column_7_cat_x    category      \n",
      " 8   column_8_cat_x    category      \n",
      " 9   column_9_date_x   datetime64[ns]\n",
      " 10  column_0_cat_y    category      \n",
      " 11  column_2_float_y  float16       \n",
      " 12  column_3_int_y    int8          \n",
      " 13  column_4_float_y  float16       \n",
      " 14  column_5_date_y   datetime64[ns]\n",
      " 15  column_6_int_y    int8          \n",
      " 16  column_7_cat_y    category      \n",
      " 17  column_8_cat_y    category      \n",
      " 18  column_9_date_y   datetime64[ns]\n",
      "dtypes: category(6), datetime64[ns](4), float16(4), int32(1), int8(4)\n",
      "memory usage: 148.0 MB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info(memory_usage = \"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8134342b",
   "metadata": {},
   "source": [
    "#### b. Loading the data in chunks\n",
    "    \n",
    "  Loading the data in chunks could be useful when dealing with large datasets. By loading data in smaller portions, or           \"chunks\", memory usage is kept to a minimum, preventing potential slowdowns or crashes that could occur if the system runs     out of memory. This could be especially beneficial in environments where memory resources are limited. Also it gives the       opportunity to process the chunks independently. Hence, more flexible data processing is possible.\n",
    "  \n",
    "  However processing the data in chunks might end up in increased total processing time due to repeated disk operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f56544",
   "metadata": {},
   "source": [
    "##### ``fetchall()`` vs ``read_sql()``\n",
    "   The both functions load the data into memory at once. Pandas' ``read_sql()`` loads the data as a DataFrame, ``fetchall()``      of ``pyodbc`` or similar libraries load the data as a list of tuples. ``fetchall()`` could potentially use more memory when    the data contains columns with mixed types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7c730b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get current memory usage\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024**2  # return memory usage in MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bbf509",
   "metadata": {},
   "source": [
    "The ``get_memory_usage`` function returns the Resident Set Size (RSS) of the current process. RSS is the portion of the process's memory that is held in RAM.\n",
    "\n",
    "When a process is started, the operating system allocates a certain amount of physical memory (RAM) for it. This memory space is divided into several segments, each with a specific purpose:\n",
    "\n",
    "The RSS value refers to the portion of this memory which is in RAM, i.e., it includes the size of the stack, the heap, and the data segment of the process. It excludes memory that is swapped out to disk or memory-mapped files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "905af372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before loading data: 1466.52 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harun\\anaconda3\\lib\\site-packages\\pandas\\io\\sql.py:762: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after loading chunk 1: 1525.12 MB\n",
      "Chunk 1 size: 10.70 MB\n",
      "Memory usage after loading chunk 2: 1539.97 MB\n",
      "Chunk 2 size: 9.69 MB\n",
      "Memory usage after loading chunk 3: 1534.78 MB\n",
      "Chunk 3 size: 4.09 MB\n",
      "Memory usage after combining all chunks: 1537.83 MB\n",
      "Final DataFrame size: 24.48 MB\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import psutil\n",
    "\n",
    "# connect to the SQL Server database\n",
    "conn_str = (\n",
    "    r'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "    r'SERVER=localhost;'\n",
    "    r'DATABASE=AdventureWorks2019;'\n",
    "    r'Trusted_Connection=yes;'\n",
    ")\n",
    "cnxn = pyodbc.connect(conn_str)\n",
    "\n",
    "# define SQL query\n",
    "query1 = \"SELECT * FROM Sales.SalesOrderDetail\"\n",
    "\n",
    "# specify chunk size\n",
    "chunk_size = 50000\n",
    "\n",
    "# initialize an empty list to store chunks\n",
    "chunks = []\n",
    "\n",
    "print(f\"Memory usage before loading data: {get_memory_usage():.2f} MB\")\n",
    "\n",
    "# read and process data in chunks\n",
    "chunk_number = 0\n",
    "for chunk in pd.read_sql(query1, cnxn, chunksize=chunk_size):\n",
    "    # print memory usage for each chunk\n",
    "    chunk_number += 1\n",
    "    print(f\"Memory usage after loading chunk {chunk_number}: {get_memory_usage():.2f} MB\")\n",
    "    print(f\"Chunk {chunk_number} size: {chunk.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "    # process each chunk as a separate dataframe if needed\n",
    "    # here we simply add it to the list\n",
    "    chunks.append(chunk)\n",
    "\n",
    "# Combine all chunks into one DataFrame\n",
    "df1 = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "print(f\"Memory usage after combining all chunks: {get_memory_usage():.2f} MB\")\n",
    "print(f\"Final DataFrame size: {df1.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "bb578292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after fetchall(): 1565.75 MB\n",
      "df1 memory 20462.578125\n",
      "After creating DataFrame from fetchall() results: 1568.20 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harun\\anaconda3\\lib\\site-packages\\pandas\\io\\sql.py:762: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df2 memory 20953680\n",
      "Memory usage after read_sql(): 1581.30 MB\n"
     ]
    }
   ],
   "source": [
    "# using fetchall()\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(\"SELECT * FROM Person.Person\")\n",
    "results = cursor.fetchall()\n",
    "print(f\"Memory usage after fetchall(): {get_memory_usage():.2f} MB\")\n",
    "\n",
    "# create dataframe from the results\n",
    "df1 = pd.DataFrame.from_records(results, columns=[desc[0] for desc in cursor.description])\n",
    "print(\"df1 memory\", df1.memory_usage(deep = True).sum()/1024)\n",
    "print(f\"After creating DataFrame from fetchall() results: {get_memory_usage():.2f} MB\")\n",
    "\n",
    "# using pandas.read_sql()\n",
    "df2 = pd.read_sql(\"SELECT * FROM Person.Person\", connection)\n",
    "print(\"df2 memory\", df2.memory_usage(deep = True).sum())\n",
    "print(f\"Memory usage after read_sql(): {get_memory_usage():.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3a70f2",
   "metadata": {},
   "source": [
    "## 2. Using NumPy arrays vs Pandas DataFrames\n",
    "\n",
    "A NumPy array is more memory-efficient than a pandas DataFrame. This is because a DataFrame has additional overhead due to its index and column label structures, as well as its ability to hold heterogeneous data types. A DataFrame essentially contains an underlying NumPy array, but also includes other data structures to support its extended functionality. Thus, if you have a large dataset composed of uniform data types and do not require the advanced functionalities provided by pandas, using a NumPy array could reduce your memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aa83ee",
   "metadata": {},
   "source": [
    "In the following script, the idea is to store the columns of the pandas dataframe as numpy arrays which reduces the memory usage drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "98e85dcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent creating numpy_arrays: 0.03900456428527832 seconds\n",
      "Memory usage of numpy array Unnamed: 0: 104 bytes\n",
      "Memory usage of numpy array column_0_cat: 104 bytes\n",
      "Memory usage of numpy array column_1_int: 104 bytes\n",
      "Memory usage of numpy array column_2_float: 104 bytes\n",
      "Memory usage of numpy array column_3_int: 104 bytes\n",
      "Memory usage of numpy array column_4_float: 104 bytes\n",
      "Memory usage of numpy array column_5_date: 40000104 bytes\n",
      "Memory usage of numpy array column_6_int: 104 bytes\n",
      "Memory usage of numpy array column_7_cat: 104 bytes\n",
      "Memory usage of numpy array column_8_cat: 104 bytes\n",
      "Memory usage of numpy array column_9_date: 104 bytes\n",
      "\n",
      "Time spent creating df2: 0.007978677749633789 seconds\n",
      "\n",
      "Time spent creating df3: 0.38344287872314453 seconds\n",
      "\n",
      "Memory usage of original df: 1426.738751411438 mbytes\n",
      "\n",
      "Memory usage of new df2: 38.14826965332031 mbytes\n",
      "\n",
      "Memory usage of new df3: 1736.6516065597534 mbytes\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import time\n",
    "#df = optimize(df)\n",
    "\n",
    "# Create a dictionary to store numpy arrays\n",
    "numpy_arrays = {}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Iterate over columns and create numpy arrays\n",
    "for column in df.columns:\n",
    "    # Note the additional list() wrapping\n",
    "    numpy_arrays[column] = list([df[column].to_numpy()])\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time spent creating numpy_arrays: {end_time - start_time} seconds \\n\")\n",
    "\n",
    "# Print memory usage of each numpy array\n",
    "for name, array in numpy_arrays.items():\n",
    "    print(f\"Memory usage of numpy array {name}: {getsizeof(array[0])} bytes\")\n",
    "    \n",
    "start_time = time.time()\n",
    "\n",
    "# Create a new DataFrame with numpy arrays at every column as single cells\n",
    "df2 = pd.DataFrame(numpy_arrays)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nTime spent creating df2: {end_time - start_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Creating a new DataFrame from the numpy arrays, with the same dtypes of columns of df\n",
    "df3 = pd.DataFrame({col: pd.Series(arr[0]) for col, arr in numpy_arrays.items()})\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nTime spent creating df3: {end_time - start_time} seconds\")\n",
    "\n",
    "# Compare memory usage\n",
    "print(f\"\\nMemory usage of original df: {df.memory_usage(deep=True).sum() / 1024**2} mbytes\")\n",
    "print(f\"\\nMemory usage of new df2: {df2.memory_usage(deep=True).sum() / 1024**2} mbytes\")\n",
    "print(f\"\\nMemory usage of new df3: {df3.memory_usage(deep=True).sum() / 1024**2} mbytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419f70b2",
   "metadata": {},
   "source": [
    "Although a dataframe as numpy arrays consume little memory, since there is no straightforward way of perform pandas operations for data manipulation using numpy arrays, we need to create a dataframe from the numpy arrays again at some point. \n",
    "\n",
    "Converting data between pandas and NumPy involves overhead, both in terms of computational resources and in terms of code complexity. \n",
    "\n",
    "We can see that it takes a considerable amount of time when creating df3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a797f030",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 1 (5000000 rows, 1 column of integers):\n",
      "\tPandas memory_usage: 39062.625 bytes\n",
      "\n",
      "DataFrame 2 (5000000 rows, 1 column of single-element numpy array):\n",
      "\tPandas memory_usage: 19531.375 bytes\n",
      "\n",
      "DataFrame 3 (1 row, 1 column of numpy array with 5000000 elements):\n",
      "\tPandas memory_usage: 19531.484375 bytes\n",
      "\n",
      "DataFrame 3 (1 row, 1 column of numpy array with 5000000 elements):\n",
      "\tPandas memory_usage: 9765.859375 bytes\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame 1: 500000 rows, 1 column of integer type\n",
    "df1 = pd.DataFrame({'col': range(5000000)})\n",
    "df1_memory_usage = df1.memory_usage(deep=True).sum()\n",
    "df1_sys_memory_usage = getsizeof(df1)\n",
    "print(f\"DataFrame 1 (5000000 rows, 1 column of integers):\")\n",
    "print(f\"\\tPandas memory_usage: {df1_memory_usage/1024} bytes\")\n",
    "\n",
    "# Create DataFrame 2: 5000000 rows, 1 column of single-element NumPy array\n",
    "arr = np.array(range(5000000))\n",
    "df2 = pd.DataFrame({'col': arr})\n",
    "df2_memory_usage = df2.memory_usage(deep=True).sum()\n",
    "df2_sys_memory_usage = getsizeof(df2)\n",
    "print(f\"\\nDataFrame 2 (5000000 rows, 1 column of single-element numpy array):\")\n",
    "print(f\"\\tPandas memory_usage: {df2_memory_usage/1024} bytes\")\n",
    "\n",
    "# Create DataFrame 3: 1 row, 1 column with a NumPy array with 5000000 elements\n",
    "df3 = pd.DataFrame({'col': [np.array(range(5000000))]})\n",
    "df3_memory_usage = df3.memory_usage(deep=True).sum()\n",
    "df3_sys_memory_usage = getsizeof(df3)\n",
    "print(f\"\\nDataFrame 3 (1 row, 1 column of numpy array with 5000000 elements):\")\n",
    "print(f\"\\tPandas memory_usage: {df3_memory_usage/1024} bytes\")\n",
    "\n",
    "# Converting the dtype of the np array of df3 to int16\n",
    "df3['col'] = df3['col'].apply(lambda x: x.astype('int16'))\n",
    "df3_memory_usage = df3.memory_usage(deep=True).sum()\n",
    "df3_sys_memory_usage = getsizeof(df3)\n",
    "print(f\"\\nDataFrame 3 (1 row, 1 column of numpy array with 5000000 elements):\")\n",
    "print(f\"\\tPandas memory_usage: {df3_memory_usage/1024} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e58b5b2",
   "metadata": {},
   "source": [
    "## 3. Dask DataFrames vs Pandas DataFrames\n",
    "\n",
    "Dask DataFrames are a large parallel DataFrame composed of smaller Pandas DataFrames. The large DataFrame is partitioned into several smaller chunks, where each chunk is a valid DataFrame itself. This allowes for distributed computation behind the scenes. Dask DataFrames support a large subset of the Pandas API, including groupbys, join operations, and sophisticated time series manipulations. Importantly, Dask operations are lazily evaluated, meaning computations are not executed until the result is explicitly requested. \n",
    "\n",
    "__Using Dask and Pandas Interchangeably:__ This can be a powerful strategy for dealing with memory limitations. The reason is Dask allows lazy evalutaion, which means computations are not performed until necessary, hence saving memory.  the ``compute()`` method is where all the computations take place. This can potentially save a lot of memory because data isn't loaded until necessary.\n",
    "\n",
    "Whenever an operation that is not supported by Dask is required, the Dask DataFrame can be converted to a Pandas DataFrame. After performing the operation, the result can be converted back into a Dask DataFrame. This method leverages the strengths of both libraries, while avoiding memory overflow issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb47045",
   "metadata": {},
   "source": [
    "__Some fundamental Dask DataFrame arguments:__ The ``npartitions`` parameter specifies how many partitions you want to divide your Dask DataFrame into. For example, if you set ``npartitions=5``, your Dask DataFrame will consist of 5 smaller Pandas DataFrames. Bear in mind that having too few partitions could limit parallelism, having too many partitions can lead to slow task scheduling and increased memory usage.\n",
    "\n",
    "In general, a good rule of thumb is to create partitions that are at least a few tens of megabytes in size, up to a maximum size that fits comfortably in memory. You might start with npartitions equal to twice the number of your machine's CPU cores and then adjust as necessary based on the memory usage and computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39455f02",
   "metadata": {},
   "source": [
    "Here is a script, comparing the time spent in operations where we retrieve data and perform merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "79554365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harun\\anaconda3\\lib\\site-packages\\pandas\\io\\sql.py:762: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to load data into pandas dataframes and convert to dask df: 2.1651906967163086 seconds\n",
      "Time taken to merge dask dataframes: 0.029373884201049805 seconds\n",
      "Time taken to convert merged dask dataframe back to pandas: 0.3315107822418213 seconds\n",
      "\n",
      "Total time taken in dask: 2.5260753631591797 seconds\n",
      "\n",
      "Time taken to load and merge dataframes using only pandas: 2.13199520111084 seconds\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import time\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "dask_total_time = 0\n",
    "\n",
    "# SQL Server connection string\n",
    "conn_str = (\n",
    "    r'mssql+pyodbc:///?odbc_connect=' +\n",
    "    r'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "    r'SERVER=localhost;'\n",
    "    r'DATABASE=AdventureWorks2019;'\n",
    "    r'Trusted_Connection=yes;'\n",
    ")\n",
    "\n",
    "engine = create_engine(conn_str)\n",
    "\n",
    "# Load the data into pandas dataframes\n",
    "start = time.time()\n",
    "query1 = \"SELECT * FROM Sales.SalesOrderDetail\"\n",
    "#df1 = pd.read_sql(query1, engine)\n",
    "ddf1 = dd.from_pandas(pd.read_sql(query1, engine), npartitions=5)\n",
    "\n",
    "query2 = \"SELECT * FROM Sales.SalesOrderHeader\"\n",
    "#df2 = pd.read_sql(query2, engine)\n",
    "ddf2 = dd.from_pandas(pd.read_sql(query2, cnxn), npartitions=5)\n",
    "end = time.time()\n",
    "print(f\"Time taken to load data into pandas dataframes and convert to dask df: {end-start} seconds\")\n",
    "dask_total_time += (end-start)\n",
    "\n",
    "# Merge operation in Dask\n",
    "start = time.time()\n",
    "merged_ddf = dd.merge(ddf1, ddf2, on='SalesOrderID', how='left')\n",
    "end = time.time()\n",
    "print(f\"Time taken to merge dask dataframes: {end-start} seconds\")\n",
    "dask_total_time += (end-start)\n",
    "\n",
    "# Convert merged dask dataframe back to pandas\n",
    "start = time.time()\n",
    "merged_df = merged_ddf.compute()\n",
    "end = time.time()\n",
    "print(f\"Time taken to convert merged dask dataframe back to pandas: {end-start} seconds\")\n",
    "dask_total_time += (end-start)\n",
    "\n",
    "print()\n",
    "print(f\"Total time taken in dask: {dask_total_time} seconds\")\n",
    "print()\n",
    "\n",
    "# Convert merged dataframe to dask dataframe\n",
    "#start = time.time()\n",
    "#ddf_merged = dd.from_pandas(merged_df, npartitions=2)\n",
    "#end = time.time()\n",
    "#print(f\"Time taken to convert merged pandas dataframe to dask: {end-start} seconds\")\n",
    "\n",
    "# Doing the same operation with pandas only\n",
    "start = time.time()\n",
    "query1 = \"SELECT * FROM Sales.SalesOrderDetail\"\n",
    "df1_pandas = pd.read_sql(query1, engine)\n",
    "\n",
    "query2 = \"SELECT * FROM Sales.SalesOrderHeader\"\n",
    "df2_pandas = pd.read_sql(query2, engine)\n",
    "\n",
    "merged_df_pandas = pd.merge(df1_pandas, df2_pandas, on='SalesOrderID', how='left')\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken to load and merge dataframes using only pandas: {end-start} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3262ac19",
   "metadata": {},
   "source": [
    "Using dask and pandas interchangeably is a little bit longer, but we load the data into memory as a whole only when it is necessary with ``compute()``. Reducing the probability of using too much memory during concurrent operations (say you have lots of requests coming in and memory usage of different operations add up and exceed the limit).\n",
    "\n",
    "__Note that it could be faster if we load the data directly into a dask dataframe using ``read_sql_table``.__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
